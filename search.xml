<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>时间序列阅读笔记</title>
      <link href="/2023/10/28/brockwell/"/>
      <url>/2023/10/28/brockwell/</url>
      
        <content type="html"><![CDATA[<h1 id="时间序列阅读笔记">时间序列阅读笔记</h1><h2 id="引言">引言</h2><h3 id="通用的时间序列建模方法">通用的时间序列建模方法</h3><p>操作步骤：</p><ul><li>画出主要feature的图，并查看<ul><li>是否有一个趋势(Trend)</li><li>是否有周期的成分(Period)</li><li>是否有突变(Sharp change)</li><li>是否有异常值(Outlier)</li></ul></li><li>移除趋势和周期项，获得一个接近静态的时间序列</li><li>建模拟合剩余的残差项</li><li>预测残差，并加上趋势和周期项，获得最终的预测值</li></ul><h3 id="静态模型">静态模型</h3><blockquote><p><strong>Def</strong>: Weakly Stationary A time series <span class="math inline">\({X_t}\)</span> is called weakly stationary, if 1.<span class="math inline">\(\mu_X(t)\)</span> is independent of time<span class="math inline">\(t\)</span> 2. <span class="math inline">\(\gamma_X(t+h,t)\)</span> is independent of time<span class="math inline">\(t\)</span> for each <span class="math inline">\(h\)</span>, where <span class="math inline">\(\gamma_X(r, s) = Cov(X_r, X_s)\)</span></p></blockquote><p>对于一个静态序列来说，<span class="math inline">\(\gamma_X(h):=\gamma_X(0,h)=\gamma_X(t,t+h)\)</span>,此时，我们称<span class="math inline">\(\gamma_X(h)\)</span>为自协方差(auto covariance function).</p><blockquote><p><strong>Def</strong>: AutoCorrelation Function(自相关函数) <span class="math inline">\(\rho_X(h) =\frac{\gamma_X(h)}{\gamma_X(0)}\)</span></p></blockquote><h3 id="估计趋势和周期">估计趋势和周期</h3><p>假设一个时间序列由三个部分构成： <span class="math display">\[X_t = m_t + s_t + Y_t\]</span> 其中<span class="math inline">\(m_t,s_t\)</span>分别表示确定性的趋势项，周期项。<span class="math inline">\(Y_t\)</span>剩余的一个带有随机性的静态序列。这节我们简单介绍一下一些简单的方法估计<span class="math inline">\(m_t, s_t\)</span></p><h4 id="只有趋势项的时候">只有趋势项的时候</h4><p>如果我们的模型只有趋势项，如下 <span class="math display">\[X_t = m_t + Y_t\]</span> 那么常见的用来估计趋势项的方法：</p><ul><li>移动平均平滑<ul><li>等权平均</li><li>指数平均</li></ul></li><li>过滤高频成分<ul><li>傅立叶变换</li><li>其他的谱分析方法</li></ul></li><li>参数化模型并拟合<ul><li>多项式拟合</li><li>机器学习，深度学习方法</li></ul></li></ul><p>当然，我们也可以反其道而行，除了我们通过上述的方法，去获取一个预测趋势的模型，我们也可以试图消除趋势，直接获得静态序列<span class="math inline">\(Y_t\)</span>。 消除趋势最常见的方法则是： -差分</p><blockquote><p><strong>Def</strong>: Lag operator <span class="math display">\[ BX_t= X_{t-1} \]</span></p><p><strong>Def</strong>: Difference operator <span class="math display">\[\nabla = 1 - B\]</span></p></blockquote><p>因此如果<span class="math inline">\(m_t\)</span>是一个<span class="math inline">\(k\)</span>阶的多项式，那么<span class="math inline">\(\nabla^k X_t\)</span> 则足以消除趋势。</p><h4 id="同时有趋势项和周期项">同时有趋势项和周期项</h4><p>上面介绍完了，在只有趋势项的是，我们如何去处理一个时间序列，这边我们考虑的模型更加完整一点，即趋势项和周期项同时存在。假设周期为<span class="math inline">\(d\)</span>,一般我们可以遵从下面的步骤：</p><ul><li>使用当前时间点附近<span class="math inline">\(d\)</span>个时间的数据做移动平均，大概估计出一个<span class="math inline">\(\hat{m}_t\)</span></li><li>对于周期内的任意一个时间点<span class="math inline">\(k, 1 \le k \led\)</span>, 计算<span class="math inline">\(\{x_{k+jd}-\hat{m}_{k+jd}\},1 \le k+jd \le n\)</span>的平均，计作<span class="math inline">\(w_t\)</span>,为了保证周期震荡总体围绕0点震荡，我们可以使用<span class="math inline">\(\hat{s}_t = w_t - \frac{1}{d}\sum_{k=1}^dw_k\)</span> 作为一个不错的周期项估计</li><li>现在我们获得了<strong>去周期</strong>的数据<span class="math inline">\(d_t = x_t -\hat{s}_t\)</span>,问题则变成了，上面只有趋势项的问题了，我们就可以上一小段列举的方法解决这个问题了。</li></ul><h2 id="stationary-process">Stationary Process</h2><blockquote><p><strong>Def</strong>: Nonnegative definite A real-valued function<span class="math inline">\(\kappa\)</span> defined on integers isnon-negative definite, if <span class="math display">\[\sum_{i,j}a_i \kappa(i-j) a_j \ge 0\]</span> for all positive integer <span class="math inline">\(i,j\)</span> and vector <span class="math inline">\(a=(a_1, a_2, \cdots)^T\)</span></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据分析，机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Portfolio Optimization</title>
      <link href="/2023/10/28/port_opt/"/>
      <url>/2023/10/28/port_opt/</url>
      
        <content type="html"><![CDATA[<h1 id="portfolio-optimization">Portfolio Optimization</h1><h2 id="简介">简介</h2><p>投资组合的优化是量化投资中非常重要的一个环节。假设我们有<span class="math inline">\(N\)</span>支资产，每个资产收集了<span class="math inline">\(T\)</span>时间的数据，用 <span class="math inline">\(X \in \mathrm{R}^{N \times T}\)</span>表示整个风险收益样本。在量化投资中最核心的两个问题就是：</p><ul><li>给定历史轨迹的条件下，预测未来时间的收益 <span class="math inline">\(\mathbb{E}[R_{t+1}|\mathcal{F}_{t}]\)</span>。</li><li>给定历史轨迹的条件下，预测未来时间的各个资产之间的correlation,例如:<span class="math inline">\(\Sigma_{t+1} | \mathcal{F}_{t}\)</span>。</li></ul><p>如果能够很好得完成第一个任务，则就可以追求到人们梦寐以求的超额收益，于此我们也可以想象第一个问题的难度是如何地高，也超过了本篇Blog所想要讨论的范畴。</p><p>我们这边主要会将讨论的内容局限在第二个问题上，假设存在一个上帝已经帮助我们解决了第一个问题，即我们已经拥有了一个相对不错的对未来收益估计的算法，使用记号<span class="math inline">\(\hat{\mu}_{t+1}\)</span> 代表我们对于 $ [R_{t+1}|_{t}]$ 的估计。那么在投资组合这一步，我们往往关注的是下面这个优化问题，<span class="math display">\[\max_{w} \hat{\mu}_{t+1}^T w, \\f_i(\hat{\mu}_{t+1}, \hat{\Sigma}_{t+1}, w) \le 0, i = 1,2,\cdots, k \\g_j(\hat{\mu}_{t+1}, \hat{\Sigma}_{t+1}, w) = 0, j = 1,2,\cdots, l\]</span></p><p>用通俗的语言描述，就是在风险可控并且符合法规(例如只允许做多)的条件下，去最大化预期收益。想要控制的风险和其他相关的约束都是用<span class="math inline">\(f,g\)</span>两种形式描述。那么为了很好地解决这个问题，很明显我们有两个问题需要解决：</p><ul><li>准确估计 <span class="math inline">\(\hat{\Sigma}_{t+1}\)</span>，即上面的第二个任务</li><li>开发一个高效的优化算法，解决上面这个优化问题。</li></ul><h2 id="correlation估计">Correlation估计</h2><p>在这个部分我们将会介绍一下，关于Correlation估计的一些基本的算法。针对这个问题的建模假设，也是被分成了两部分。第一，假设Correlation是静态的；第二，假设Correlation是动态。第一种情况下面，我们拥有一些比较良好的数学保障，但是和现实背离得稍微远一点。第二种情况，则是和现实更近，但是解决该问题的难度也会高上很多。</p><h3 id="静态correlation估计">静态Correlation估计</h3><p>对于静态的Correlation的估计，首先我们会想到一个最基本的方法，就是用样本的协方差矩阵估计全体的协方差矩阵。所谓的样本协方差矩阵，可以写成下面的形式:<span class="math display">\[S = \frac{1}{T}(XX^T - \frac{1}{T}X\mathbf{1}\mathbf{1}^TX) =\frac{1}{T}X(\mathbf{I} - \frac{1}{T}\mathbf{1}\mathbf{1}^T)X^T\]</span> 从这个表达式，我们就可以看出来一个较为严重的问题, <span class="math inline">\(S\)</span>的rank不会高于<span class="math inline">\(\mathbf{I} -\frac{1}{T}\mathbf{1}\mathbf{1}^T\)</span>, 即不会高于<span class="math inline">\(T\)</span>。那么当你所采集到的数据时间窗口<span class="math inline">\(T\)</span>和<span class="math inline">\(N\)</span>在同一个量级，甚至更低的时候，该估计矩阵很有可能是singluar的，这会极大的影响到后续优化问题的求解。该问题在现实投资中还是比较明显的，尤其是当你的投资频率较低时，该问题会越发显著。那么主流的解决上面这个问题的方法也分成两大主流的流派，当然这两个流派也不完全独立，也是可以混合使用。</p><ul><li>Shrinkage方法</li><li>Multi-factor估计方法</li></ul><h4 id="shrinkage">Shrinkage</h4><p>Shrinkage的想法非常的简单，就是既然<span class="math inline">\(S\)</span>本身作为<span class="math inline">\(\Sigma\)</span>的估计在上述 <span class="math inline">\(N,T\)</span>比较接近的时候有一些问题，那么我们可以再额外另外一个估计 <span class="math inline">\(F\)</span>, 使用这两个估计的组合，例如<span class="math inline">\((1-\delta)S + \delta F\)</span>, 作为<span class="math inline">\(\Sigma\)</span>的估计呢，这边 <span class="math inline">\(\delta\)</span>是一个描述shrinkage程度的参数。在具体讨论这个<span class="math inline">\(F\)</span>之前，我们可以思考一下引入这个<span class="math inline">\(F\)</span>, 我们需要解决什么，即期望<span class="math inline">\(F\)</span>能够拥有哪些性质。下面是我自己个人的一些想法：</p><p><strong><em>首先，<span class="math inline">\(S\)</span>矩阵对于<span class="math inline">\(\Sigma\)</span>的估计无偏的，但是如果样本量不够的情况下面，其会具有很高的variance，即统计学习里，常说的 bias-variance tradeoff。那么很显然，我们是希望引入的<span class="math inline">\(F\)</span>对于 biasness的容忍度可以高一点，但是希望其有更低的 variance。除此之外，我们期望<span class="math inline">\(F\)</span>的引入可以缓解 singularity的问题。</em></strong></p><p>虽然<span class="math inline">\(F\)</span>确实构造的自由度很高，你可以选择任何的方法去构造一个合适的估计算子，但是就个人理解来说，上面这段话应当做一个基本的参考准则。我想这也是Ledoit,Wolf在他们最初的两个工作里选择 <span class="math inline">\(F\)</span>的原则<a href="#ledoit-1"><sup>1,<sup></sup></sup></a><a href="#ledoit-2"><sup>2<sup></sup></sup></a></p><ul><li>认为任何两个股票之间的correlation都是独立的，采取全局平均的相关程度作为<span class="math inline">\(F\)</span></li><li>采用单因子模型去估计协方差矩阵</li></ul><h5 id="最优的-delta">最优的 <span class="math inline">\(\delta\)</span></h5><p>这边我们会介绍一下如何获取最优的 <span class="math inline">\(\delta\)</span>。根据上面的介绍，我们知道找到最优的<span class="math inline">\(\delta^*\)</span> 等价于求解下面这个问题<span class="math display">\[\min_{\delta} \mathbb{E}\|(1-\delta) S + \delta F - \Sigma\|_2^2\]</span> 该问题的解为 <span class="math display">\[\delta^* = \frac{\sum_{ij}\mathbb{Var}[s_{ij}]- \mathbb{Cov}[f_{ij},s_{ij}]}{\sum_{ij}\mathbb{Var}[s_{ij}-f_{ij}] +(\phi_{ij}-\sigma_{ij})^2}\]</span> 其中 <span class="math inline">\(\Phi\)</span>为构造出来的估计算子 <span class="math inline">\(F\)</span>的真实值。具体可以参考附录<a href="#opt_delta">1</a></p><p>很显然这个式子有两个问题：</p><ul><li>依赖<span class="math inline">\(F,S\)</span>的真实统计，方差和协方差</li><li>依赖真实未知的 <span class="math inline">\(\Sigma,(\sigma_{ij})\)</span></li></ul><p>对于第一个问题，我们考虑在<span class="math inline">\(T\)</span>比较大的时候，真实统计和样本估计之间的误差是什么样子的。如果误差较小，那么我们则可以对于<span class="math inline">\(\delta^*\)</span> 构造一个渐进有效的表达式。首先对于第一个问题来说，中心极限定理的直觉告诉我们，无论是<span class="math inline">\(\mathbb{Var}[s_{ij}],\mathbb{Cov}[f_{ij}, s_{ij}],\mathbb{Var}[s_{ij}-f_{ij}]\)</span> 其中哪一个，他跟随<span class="math inline">\(T\)</span>的scaling应该都是在 <span class="math inline">\(\frac{1}{T}\)</span> 这量级上，也是就说 <span class="math inline">\(\delta^* \propto \frac{\alpha}{T}\)</span>,那么一个最合理，也是最符合直觉的猜测，就是我们可以认为： <span class="math display">\[\delta^* = \frac{1}{T}\frac{\pi - \rho}{\gamma} + o(\frac{1}{T^2})\]</span> 这边, <span class="math display">\[\pi = \lim_{T \to \infty} \sum_{ij} \mathbb{Var}[\sqrt{T}s_{ij}],\\\rho = \lim_{T \to \infty} \sum_{ij} \mathbb{Cov}[\sqrt{T}f_{ij},\sqrt{T}s_{ij}],\\\gamma = \sum_{ij}(\phi_{ij} - \sigma_{ij})^2\]</span> 严格的数学分析可以参考附录<a href="#opt_delta">1</a>。</p><p>这边我们对 <span class="math inline">\(\delta^*\)</span>做一些简要的说明，因为 <span class="math inline">\(\delta^* \sim\frac{1}{T}\)</span>, 那么我们可以知道 <span class="math inline">\(T \to\infty\)</span> 的时候，<span class="math inline">\(\delta^* \to0\)</span>, 整个估计收敛到 <span class="math inline">\(S\)</span>,这也是和我们统计学上所学到的知识是吻合的。当我们拥有足够多的样本是，使用样本协方差矩阵作为真实协方差矩阵的估计是一个有效的估计。</p><h4 id="multi-factor">Multi-factor</h4><h3 id="动态correlation估计">动态Correlation估计</h3><h2 id="优化算法">优化算法</h2><h2 id="附录">附录</h2><h3 id="opt_delta">最优的 <span class="math inline">\(\delta\)</span></h3><p><span class="math display">\[\|(1-\delta) S + \delta F - \Sigma\|_2^2 = \sum_{i,j=1}^N\left((1-\delta) s_{ij} + \delta f_{ij} - \sigma_{ij}\right)^2 \\= \sum_{i,j=1}^N \delta^2 \left(s_{ij}^2 + f_{ij}^2 -2s_{ij}f_{ij}\right) - 2\delta \left(s_{ij}^2 +f_{ij}\sigma_{ij}-s_{ij}\sigma_{ij}-s_{ij}f_{ij}\right) + C\]</span>因为上式为一个开口向上的二次函数，最后一项跟极值点没有太大关系。同时我们可以发现<span class="math display">\[\delta^* = \frac{\sum_{ij}\mathbb{E}\left[s_{ij}^2 + f_{ij}\sigma_{ij}-s_{ij}\sigma_{ij}-s_{ij}f_{ij}\right]}{\sum_{ij}\mathbb{E}[s_{ij}^2 +f_{ij}^2 - 2s_{ij}f_{ij}]}\]</span> 因为<span class="math inline">\(S\)</span>是一个无偏的估计,因此 <span class="math inline">\(\mathbb{E}[S] = \Sigma\)</span>,同时我们假设构造出来的估计 <span class="math inline">\(F\)</span>在统计意义上也会收敛到 <span class="math inline">\(\Phi\)</span>。那么我们可以知道， <span class="math display">\[\delta^* = \frac{\sum_{ij}\mathbb{Var}[s_{ij}]- \mathbb{Cov}[f_{ij},s_{ij}]}{\sum_{ij}\mathbb{Var}[s_{ij}-f_{ij}] +(\phi_{ij}-\sigma_{ij})^2}\]</span> 下面我们将以 <span class="math inline">\(\sum_{ij}\mathbb{Var}[\sqrt{T}s_{ij}]\)</span>为例证明其渐近收敛到 <span class="math inline">\(\pi\)</span>，以此说明三个二次项的scaling均为<span class="math inline">\(\frac{1}{T}\)</span>。 显然<span class="math inline">\(s_{ij}=\frac{1}{T}\sum_t x_{it}x_{jt} -\frac{1}{T}\sum_tx_{it}\frac{1}{T}\sum_{t}x_{jt}\)</span>。对于第二项，中心极限定理告诉我们<span class="math inline">\(\sqrt{T}(\frac{1}{T}\sum_{t}x_{it} -\mathbb{E}[x_i]) \to \mathcal{N}(0, \mathbb{Var}[x_i])\)</span>,假设<span class="math inline">\(X\)</span>中的所有的随机变量都是有限的4阶以下的moment，那么对于<span class="math inline">\(\sqrt{T}s_{ij}\)</span>中的第二项在<span class="math inline">\(T\)</span>很大的时候，则不会有实质的贡献，因此我们只需要把重点放在第一项。对于第一项，我们还知道<span class="math inline">\(\sigma_{ij}=\mathbb{E}[x_ix_j]-\mathbb{E}[x_i]\mathbb{E}[x_j]\)</span>是一个确定性的数字，对比两项的差距，我们发现只要分析<span class="math inline">\(\frac{1}{T}\sum_{t}x_{it}x_{ij} -\mathbb{E}[x_i]\mathbb{E}[x_j]\)</span>的性质。同样，中心极限定理告诉我们<span class="math inline">\(\sqrt{T}(\frac{1}{T}\sum_{t}x_{it}x_{ij} -\mathbb{E}[x_i]\mathbb{E}[x_j] - \sigma_{ij}) \to \mathcal{N}(0,\rho_{ij})\)</span></p><h2 id="参考文献">参考文献</h2><div id="ledoit-1"><div><ul><li>[1] Ledoit, O., &amp; Wolf, M. (2003). <i>Honey, I Shrunk the SampleCovariance Matrix</i>. http://www.ledoit.net/honey.pdf</li></ul><div id="ledoit-2"><div><ul><li>[2] Ledoit, O., &amp; Wolf, M. (2003). Improved estimation of thecovariance matrix of stock returns with an application to portfolioselection. <i>Journal of Empirical Finance</i>, <i>10</i>(5), 603–621.https://doi.org/10.1016/S0927-5398(03)00007-0</li></ul></div></div></div></div><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 投资 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 量化投资 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Successful Algorithm Trading 读后感</title>
      <link href="/2023/10/28/successful_trading/"/>
      <url>/2023/10/28/successful_trading/</url>
      
        <content type="html"><![CDATA[<h1 id="successful-algorithm-trading-读后感">Successful AlgorithmTrading 读后感</h1><h2 id="概览">概览</h2><h3 id="算法交易和主观交易的对比">算法交易和主观交易的对比</h3><p>优点:</p><ul><li>提高了投研效率</li><li>没有主观输入，受情绪影响较小</li><li>评价指标更加多维</li><li>更高的交易频率，人工操作不来</li></ul><p>缺点:</p><ul><li>需要一定的资金量</li><li>需要较高的科学素养和编程水平</li></ul><h3 id="散户和机构的对比">散户和机构的对比</h3><p>散户的优势:</p><ul><li>小资金量更加灵活</li><li>对冲基金之间人员流动，会导致策略趋同，造成交易拥挤，散户的交易系统和这些关联较小</li><li>散户的交易对市场价格的冲击可以忽略</li></ul><p>散户的劣势:</p><ul><li>对杠杆的使用有更多的限制</li><li>散户的交易在券商的排队往往优先级较低，这会造成实盘和回测有较大差距</li><li>信息数据滞后</li></ul><p>散户对于风险管理相比于基金来说，会更加自由，完全取决于个人的风险偏好。这有好有坏，往往这也会让散户忽略风险管理，把绝大部分时间放在构建交易策略上。</p><p>散户不需要关注交易曲线，不需要和投资者打交道，不需要定期公布事项满足监管，不需要和同行比，不需要和被动投资比。散户只需要关注的就是绝对收益。</p><h2 id="回测">回测</h2><h3 id="为什么需要回测">为什么需要回测</h3><p>回测可以帮助做到下面几点：</p><ul><li>筛选：可以快速帮我筛选策略</li><li>建模：可以快速验证对市场的想法</li><li>优化：可以精细打磨自己的策略</li></ul><h3 id="回测过程中常见的偏差bias">回测过程中常见的偏差(Bias)</h3><p>我们通过回测慢慢迭代自己的策略的时候，往往也引入了很多的偏差(Bias)，常见的有：</p><ul><li>优化偏差(Optimisation Bias)<ul><li>过拟合数据，包括模型，策略超参数等等</li></ul></li><li>未来信息(Look ahead Bias)</li><li>幸存偏差(Survivorship Bias)<ul><li>每年有大量的股票退市，但是往往数据集里面只会包含存活至今的股票数据</li></ul></li><li>主观偏差(Cognitive Bias)<ul><li>面对回测曲线里面最大回撤在25%，可能较容易接受。但是实盘面对如此的回测往往难以忍受，因此很多策略终止在低点。</li></ul></li></ul><h3 id="交易损耗">交易损耗</h3><p>交易过程中带来的损耗包括但不限于：</p><ul><li>手续费<ul><li>印花税</li><li>佣金</li><li>...</li></ul></li><li>滑点(Slippage)<ul><li>交易信号对应的价格和实际价格之间的差距</li><li>主要因素包括：波动性，延迟，交易频率</li></ul></li><li>市场冲击</li></ul><h3 id="交易风格">交易风格</h3><p>在确定自己的交易风格之前，我们需要从下面几个方面出发： -你的性格是什么样子的 - 你所允许的交易时间是怎么样的</p><h2 id="时间序列分析">时间序列分析</h2><p>这一节关于时间序列分析的相关算法，作者讲得非常简洁，不值得记录。</p><h2 id="预测">预测</h2><p>这边作者只是简单减少了一些常见的机器学习算法，例如逻辑回归，线性回归，支持向量机，随机森林等等，以及机器学习当中常见的对于结果判别的方法。</p><h2 id="业绩归因-风险管理">业绩归因 &amp;&amp; 风险管理</h2><p>从各个level的业绩归因，可以更好地回溯整个交易系统的问题。一般可以从如下几个方面看待整个业绩的表现:</p><ul><li>策略<ul><li>策略在回测中的模拟</li><li>策略在模拟盘中的表现(模拟盘交易误差几乎可以忽略)</li></ul></li><li>交易执行<ul><li>衡量策略提供的交易信号和现实交易执行产生的误差</li></ul></li><li>投资组合管理<ul><li>是否存在更加合理的投资组合管理，可以降低交易的损耗</li></ul></li></ul><p>那么何为上述所说的"业绩"呢，一般我们需要从下面几个角度去考虑:</p><ul><li>收益<ul><li>总收益: 计算的时候需要考虑杠杆，做空等等因素</li><li>复合年化收益</li></ul></li><li>风险<ul><li>回撤(DrawDown)</li><li>收益的波动率</li></ul></li><li>风险收益<ul><li>Sharpe Ratio: <span class="math inline">\(\sqrt{N}\frac{\mathbb{E}[R_{\alpha}-R_{\beta}]}{\sqrt{\mathbb{Var}[R_{\alpha} - R_{\beta}]}}\)</span>,这边<span class="math inline">\(N\)</span>为一年内交易日数目，如果你的收益是按照天计算的话。<span class="math inline">\(R_{\alpha} - R_{\beta}\)</span>代表超额收益。其中Benchmark可以选择为对应的ETF.</li><li>Sortino Ratio: <span class="math inline">\(\sqrt{N}\frac{\mathbb{E}[R_{\alpha}-R_{\beta}]}{\sqrt{\mathbb{Var}[R_{\alpha} - R_{\beta}]}_d}\)</span>.这边的下标<span class="math inline">\(d\)</span>代表我们只考虑，当超额收益为负的时候波动率，因为亏损的波动，更加容易交易情绪的波动。</li><li>Calmar Ratio: <span class="math inline">\(\sqrt{N}\frac{\mathbb{E}[R_{\alpha}-R_{\beta}]}{\max \mathbf{drawdown}}\)</span>.</li></ul></li><li>交易指标<ul><li>PnL</li><li>Average Period PnL</li><li>Maximum Period Profit</li><li>Maximum Period Loss</li><li>Average Period Profit</li><li>Average Period Loss</li><li>Wining Period</li><li>Losing Period</li><li>Percentage Win/Loss Periods</li></ul></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 投资 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 量化投资 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Replicator Dynamics and reinforcement learning</title>
      <link href="/2023/01/01/ess_rl/"/>
      <url>/2023/01/01/ess_rl/</url>
      
        <content type="html"><![CDATA[<p><object data="./ess_rl.pdf" type="application/pdf" width="100%" height="877px"><br><a id="more"></a><p></p></object></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux/mac工作环境配置</title>
      <link href="/2023/01/01/prepare_env/"/>
      <url>/2023/01/01/prepare_env/</url>
      
        <content type="html"><![CDATA[<h1 id="Prepare-Work-Env-for-Linux"><a href="#Prepare-Work-Env-for-Linux" class="headerlink" title="Prepare Work Env for Linux"></a>Prepare Work Env for Linux</h1><p>Prepare your work environment for linux system<br><a id="more"></a></p><h2 id="Software-install"><a href="#Software-install" class="headerlink" title="Software install"></a>Software install</h2><h3 id="git"><a href="#git" class="headerlink" title="git"></a>git</h3><p>Use git with ssh style:</p><ul><li><p>generate private and public key:</p>  <figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd ~/.ssh &amp;&amp; ssh-keygen -o</span><br></pre></td></tr></tbody></table></figure><ul><li>now github only support sha-2, we could use the following code to generate <code>ssh-keygen -t ecdsa -b 521 -C "your_email"</code></li></ul></li><li>check whether it does generate <em>id_rsa</em> and <em>id_rsa.pub</em></li><li>copy your <em>id_rsa.pub</em> to your git account setting</li></ul><p><strong><em>Note</em></strong><br>If you use <code>git clone</code> timeouts, you can use <code>ssh -vT git_url</code> to check port or other information</p><h3 id="zsh"><a href="#zsh" class="headerlink" title="zsh"></a>zsh</h3><p>install oh my zsh</p><ul><li>install zsh <code>yum install zsh</code></li><li>change shell to zsh <code>chsh -s /bin/zsh</code></li><li>install oh-my-zsh </li></ul><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -c "$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)"</span><br></pre></td></tr></tbody></table></figure><h3 id="gcc"><a href="#gcc" class="headerlink" title="gcc"></a>gcc</h3><p>update gcc version</p><h4 id="centos-under-7-6"><a href="#centos-under-7-6" class="headerlink" title="centos under 7.6"></a>centos under 7.6</h4><ul><li><p>install <code>yum install centos-release-scl</code></p></li><li><p>find gcc related <code>yum list dev\*gcc</code></p></li><li><p>install gcc related <code>yum install devtoolset-x-gcc devtoolset-x-gcc-c++</code></p></li><li><p>valid temporarily <code>source /opt/rh/devtoolset-x/enable</code></p></li><li><p>check <code>gcc -v</code> and <code>g++ -v</code></p></li><li><p>valid permanently add <code>source /opt/rh/devtoolset-x-gcc-c++</code> to your bashrc or zshrc</p></li></ul><h4 id="cento-beyond-7-6"><a href="#cento-beyond-7-6" class="headerlink" title="cento beyond 7.6"></a>cento beyond 7.6</h4><p>Note that <code>devtools-x-gcc</code> has been renamed as <code>gcc-toolset-x</code></p><ul><li><p>check <code>yum list | grep gcc-toolset</code></p></li><li><p>use dnf to install <code>dnf install gcc-toolset-x</code></p></li><li><p>source to activate <code>source /opt/rh/gcc-toolset-x/enable</code></p></li></ul><h3 id="conda"><a href="#conda" class="headerlink" title="conda"></a>conda</h3><p>follow the instruction <a href="https://docs.anaconda.com/anaconda/install/linux/" target="_blank" rel="noopener">Install Conda Linux</a></p><h3 id="go"><a href="#go" class="headerlink" title="go"></a>go</h3><p>install golang</p><ul><li>download <code>wget https://go.dev/dl/go1.18.3.linux-amd64.tar.gz</code></li><li>remove the oldest go<ul><li>use <code>which go</code> to find the current go</li><li>remove <code>rm -rf $cur_go_dir</code> such as <code>rm -rf /usr/local/go</code></li></ul></li><li>decompress <code>tar -C /usr/local -xzf go1.18.3.linux-amd64.tar.gz</code></li><li>env variable <code>export PATH=$PATH:/usr/loca/go/bin</code>, add to zshrc make permanently</li><li>remove source file <code>rm go1.18.3.linux-amd64.tar.gz</code></li></ul><h3 id="protobuf"><a href="#protobuf" class="headerlink" title="protobuf"></a>protobuf</h3><ul><li>download the particular version   <figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget  https://github.com/protocolbuffers/protobuf/releases/download/v3.7.1/protobuf-cpp-3.7.1.tar.gz</span><br></pre></td></tr></tbody></table></figure></li><li><code>tar -xzvf protobuf-cpp-3.7.1.tar.gz</code></li><li>install   <figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd protobuf-3.7.1</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make check</span><br><span class="line">sudo make install</span><br><span class="line">sudo ldconfig</span><br></pre></td></tr></tbody></table></figure></li><li>to support golang <code>go install google.golang.org/protobuf/cmd/protoc-gen-go@latest</code></li></ul><h3 id="tmux"><a href="#tmux" class="headerlink" title="tmux"></a>tmux</h3><p>If you want to install tmux2, just use <code>yum</code> or <code>dnf</code></p><p>Install tmux 3.0</p><ul><li>install deps  <figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install libtermcap-devel ncurses-devel libevent-devel readline-devel</span><br></pre></td></tr></tbody></table></figure><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/tmux/tmux.git</span><br><span class="line">cd ./tmux</span><br><span class="line">git checkout 3.0</span><br><span class="line">sh autogen.sh</span><br><span class="line">./configure &amp;&amp; make</span><br><span class="line">sudo make install</span><br></pre></td></tr></tbody></table></figure></li></ul><h3 id="vim"><a href="#vim" class="headerlink" title="vim"></a>vim</h3><p>follow the instruction by <a href="git@github.com:anschen1994/vimrc.git">url</a></p><h3 id="tldr"><a href="#tldr" class="headerlink" title="tldr"></a>tldr</h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dnf install nodejs</span><br><span class="line">npm install tldr</span><br></pre></td></tr></tbody></table></figure><h3 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h3><p><strong><em>tensoflow 1.15 is not supported by python3.8 and later version. If you want to install 1.15, pls install python3.7</em></strong></p><h3 id="docker"><a href="#docker" class="headerlink" title="docker"></a>docker</h3><p>remove images store position</p><h3 id="vscode"><a href="#vscode" class="headerlink" title="vscode"></a>vscode</h3><h4 id="vscode-free-key-pass"><a href="#vscode-free-key-pass" class="headerlink" title="vscode free key pass"></a>vscode free key pass</h4><ul><li>local generate ssh-key <code>ssh-keygen -o</code></li><li>copy <code>id_rsa.pub</code> to your remote server <code>id_rsa.pub</code> and <code>authorized_keys</code></li></ul><h1 id="从零配置mac"><a href="#从零配置mac" class="headerlink" title="从零配置mac"></a>从零配置mac</h1><h2 id="系统"><a href="#系统" class="headerlink" title="系统"></a>系统</h2><p>系统快捷键:<br></p><figure class="highlight plain"><figcaption><span>```Setting```-&gt;```Keyboard```-&gt;```Keyboard Shorcuts```-&gt;```Modifier Keys```</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Safari地址栏搜索引擎修改：```Safari```-&gt;```Setting```-&gt;```Search</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="VSCode"><a href="#VSCode" class="headerlink" title="VSCode"></a>VSCode</h2><ol><li>选择使用<code>github</code>账号同步</li><li><code>vim</code>插件，repeated keys, 运行下面的指令:<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">defaults write com.microsoft.VSCode ApplePressAndHoldEnabled -bool false              # For VS Code</span><br><span class="line">defaults write com.microsoft.VSCodeInsiders ApplePressAndHoldEnabled -bool false      # For VS Code Insider</span><br><span class="line">defaults write com.visualstudio.code.oss ApplePressAndHoldEnabled -bool false         # For VS Codium</span><br><span class="line">defaults write com.microsoft.VSCodeExploration ApplePressAndHoldEnabled -bool false   # For VS Codium Exploration users</span><br><span class="line">defaults delete -g ApplePressAndHoldEnabled</span><br></pre></td></tr></tbody></table></figure>并且可以在光标移动的速度: <code>System Preference</code>-&gt;<code>Keyboard</code></li></ol><h2 id="Terminal"><a href="#Terminal" class="headerlink" title="Terminal"></a>Terminal</h2><ol><li>安装<code>iterm2</code></li><li>安装<code>homebrew</code></li><li>安装<code>oh-my-zsh</code><ul><li>安装插件<code>https://mdnice.com/writing/6774c6693d374e548b0a4434f85dceb6</code></li></ul></li><li>安装<code>tldr</code>: <code>brew install tldr</code></li><li><pre><code class="lang-LazyVim```配置:"> - ```brew install neovim</code></pre><ul><li>Follow <code>https://www.lazyvim.org/installation</code></li></ul></li><li>安装<code>tmux</code><ul><li><code>brew install tmux</code></li><li>拷贝配置文件<code>mac_tmux.conf</code></li></ul></li><li>配置vim模式: 在<code>.zshrc</code>的plugin中加入<code>vi-mode</code>插件 </li></ol><h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><ol><li>安装<code>miniconda</code></li></ol><h2 id="git-1"><a href="#git-1" class="headerlink" title="git"></a>git</h2><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd ~/.ssh &amp;&amp; ssh-keygen -t ecdsa -b 521 -C "your_email"</span><br></pre></td></tr></tbody></table></figure><h2 id="zsh-1"><a href="#zsh-1" class="headerlink" title="zsh"></a>zsh</h2><p>插件</p><ul><li><code>``zsh-autosuggestions</code></li><li><code>``zsh-syntax-highlighting</code></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> CS工程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性回归漫谈</title>
      <link href="/2022/09/11/linear_regression/"/>
      <url>/2022/09/11/linear_regression/</url>
      
        <content type="html"><![CDATA[<h1 id="线性回归漫谈">线性回归漫谈</h1><p>线性回归是一项假设因果线性的数据拟合方法，如今也是被大家广泛使用。但是其中对应underlying的条件假设都被大家忽略了，这篇文章尝试详细整理线性回归的方方面面，方便之后回顾和参考。大部分的资料都可以在参考文献中找到。<a id="more"></a></p><h2 id="目录">目录</h2><p>[TOC] ## 假设首先我们将各个后续使用到的假设条件在这个<code>section</code>列举出来:&gt; <strong>Assumption1: Linearity</strong><span id="A1"> &gt; <span class="math display">\[&gt; Y = X\beta + \epsilon, Y \in \mathbb{R}^{N \times 1}, X \in\mathbb{R}^{N \times (K+1)}, \beta \in \mathbb{R}^{(K+1) \times 1},\epsilon \in \mathbb{R}^{N \times 1}&gt; \]</span></span></p><p>这个假设是我们采取限性回归的基础，即假设因果关系符合现行表达。<span class="math inline">\(N\)</span>代表着样本数量，<span class="math inline">\(K+1\)</span>代表因变量的个数<span class="math inline">\(K\)</span>加上<span class="math inline">\(1\)</span>个<code>biasness</code>。<span class="math inline">\(\epsilon\)</span>是我们俗称的残差项，当然在金融领域，很多人也称之为特异性误差。&gt; <strong>Assumption2: Strict Exogenity</strong> &gt; <span class="math display">\[&gt; \mathbb{E}[\epsilon | X] = 0&gt; \]</span></p><p>这假设告诉我们，<span class="math inline">\(Y\)</span>能够被<span class="math inline">\(X\)</span>的线性表示充分表达。同时这个假设也意味着,<span class="math display">\[\mathbb{E}[\epsilon] = 0, \mathbb{E}[\epsilon X] = 0,\mathbb{Cov}[\epsilon, X] = 0\]</span></p><blockquote><p><strong>Assumption3: Conditional Homoskedasticity</strong> <span class="math display">\[\mathbb{Var}[\epsilon | X] = \sigma_{\epsilon}^2 I\]</span></p></blockquote><blockquote><p><strong>Assumption4: Conditionally Uncorrelated</strong> <span class="math display">\[\mathbb{Cov}(\epsilon_i, \epsilon_j | X) = 0, \forall i \neq j\]</span></p></blockquote><p>值得注意的是假设2-4，三个假设是非常强的假设。他们都要求在任意给定的<span class="math inline">\(X\)</span>条件下，<span class="math inline">\(\epsilon\)</span>的一阶和二阶统计量满足一些条件，这就意味着<span class="math inline">\(\epsilon\)</span>本身也需要满足这些条件，但是反之却不然。举个例子，<span class="math inline">\(\mathbb{E}[f(\epsilon)] = \mathbb{E}_X\left[\mathbb{E}[f(\epsilon)|X]\right]\)</span>, 当<span class="math inline">\(\mathbb{E}[f(\epsilon)|X]\)</span>是一个<span class="math inline">\(X\)</span>无关的量的时候，我们可以claim<span class="math inline">\(\mathbb{E}[f(\epsilon)]\)</span>也满足的相同的性质，但是反过来，<span class="math inline">\(\mathbb{E}[f(\epsilon)]\)</span>满足某个性质的时候，我们不能claim <span class="math inline">\(\mathbb{E}[f(\epsilon|X)]\)</span>也满足改性质</p><blockquote><p><strong>Assumption5: Linear Independence</strong> <span class="math display">\[\sum_{k=1}^K c_{k}X_{ik} = 0, \forall i=1,\cdots, N \Leftrightarrow c_k= 0, \forall k = 1, \cdots, K\]</span></p></blockquote><p>该假设则保证模型选取的因变量是线性独立的，即我们没有选取“重复”的因变量。该假设也保证了线性回归模型在数学上的可求解性，因为它意味着，<span class="math display">\[rank(X) = k + 1, \\\det{X^T X} \neq 0\]</span></p><blockquote><p><strong>Assumption6: Gaussional Res</strong> <span class="math display">\[\epsilon | X \sim \mathcal{N}(0, \sigma_\epsilon^2 I)\]</span></p></blockquote><p>残差符合高斯分布这个假设则是更加强于假设2-4，我相信有一部分人会认为该假设是线性回归的underlying的假设。我们后续回揭晓这个假设其实对于线性回归没有任何的作用。</p><p><em>下面我们详细讨论这些假设在线性回归模型的应用，并且试图慢慢抹去其中的一些，然后来看看其带来的影响，以及新的求解方法</em></p><h2 id="ols">OLS</h2><h3 id="estimator">Estimator</h3><p>假设我们需要的解决的问题满足<strong>Assumption 1-5</strong>。<strong>Assumption2-4</strong> 告诉我们可以选取<span class="math inline">\(\mathcal{L}_{OLS}:=\mathbb{E}[\epsilon^T\epsilon]\)</span> 作为优化的目标函数是合适，即我们需要解决下面的问题:<span class="math display">\[\min_{\beta} \mathcal{L}_{OLS} = \min_{\beta}\mathbb{E}\left[Y^TY-Y^TX\beta - \beta^T X^TY + \beta^T X^TXY\right]\]</span> 我们可以发现<span class="math inline">\(\frac{\partial\mathcal{L}_{OLS}}{\partial \beta} = -2X^T + 2X^TX\beta\)</span>, 因此，&gt; <strong>OLS estimator</strong> &gt; <span class="math display">\[&gt; \hat{\beta} = (X^TX)^{-1} X^T Y&gt; \]</span></p><p>回顾之前的Assumption，我们会发现<strong>Assumption5</strong>在这个时候保证了可解性。说到了这边，我们是时候引入线性回归中一个最重要的(个人观点)的定理:<strong>Gauss-Markov Theorem</strong></p><blockquote><p><strong>Theorem: Gauss-Markov</strong>在<strong>Assumption1-5</strong>都成立的条件下，OLSestimator构成一个<strong>consistent BLUE</strong>(Best Linear UnbiasedEstimator).</p></blockquote><p>我们在附录里，大概展开了其证明的一些关键。</p><h3 id="衡量ols">衡量OLS</h3><p>上面的讨论，都是在比较的理想的情况下，最优的estimator应该长成OLS这个样子。但是在实际中，我们获取的数据终究和理想会有一些偏差，那么这个应该方法来衡量OLS在这些情况下的表现。</p><h4 id="hypothesis-testing">Hypothesis Testing</h4><p>检验OLS所获得得<span class="math inline">\(\hat{\beta}\)</span>的优劣，最简单的方式就是假说检验了。这个时候我们就需要利用到<strong>Assumption6</strong>,即 <span class="math display">\[\hat{\beta}|X \sim \mathcal{N}(\beta,\sigma^2(X^TX)^{-1})\]</span> 既然<span class="math inline">\(\hat{\beta}|X\)</span>服从高斯分布，那么就经典的t-检验(单因子分析)或者F-检验(多因子分析)来做假说检验。</p><h5 id="单因子">单因子</h5><p>如果<span class="math inline">\(\hat{\beta}\)</span>只有一维，那么可以直接使用t-检验，检验<span class="math inline">\(\hat{\beta}\)</span>本身。当然这个时候，我们需要估计的<span class="math inline">\(\hat{\sigma}\)</span>去代替<span class="math inline">\(\sigma\)</span>。这样<span class="math inline">\(t:=\frac{\hat{\beta}-\beta}{\hat{\sigma}{(X^TX)^{-\frac{1}{2}}}}\)</span>服从t-分布。而这边<span class="math inline">\(\hat{\sigma}^2\)</span>则是对于<span class="math inline">\(\mathbb{Var}[\epsilon]\)</span>的估计，我们可以采用最简单的方法来估计，<span class="math display">\[\hat{\sigma}^2 = \frac{1}{N-(k+1)}\sum_{i=1}^N (Y_i - X_i \hat{\beta})^2\]</span> 这边<span class="math inline">\(N-(k+1)\)</span>则是<code>Bessel correction</code>。有了这些，我们可以将完整的假说检验描述成： &gt; <strong>SingleFactor</strong> &gt; <span class="math display">\[&gt; H_0: \hat{\beta} = \beta_0 \\&gt; H_1:\hat{\beta} \neq \beta_0&gt; \]</span> &gt; &gt; If <span class="math inline">\(t &lt;t_{\frac{\alpha}{2}}\)</span> or <span class="math inline">\(t &gt;t_{1-\frac{\alpha}{2}}\)</span>, We can reject <span class="math inline">\(H_0\)</span>, where <span class="math inline">\(\alpha\)</span> is tolerance.</p><h5 id="多因子">多因子</h5><h4 id="r-square">R-square</h4><blockquote><p><strong>R-square</strong> <span class="math display">\[R^2 := \frac{\mathrm{ESS}}{\mathrm{TSS}} = \frac{\sum_{i=1}^N (\hat{Y}_i- \bar{Y})^2}{\sum_{i=1}^N (Y_i-\bar{Y})^2} = 1 -\frac{\mathrm{RSS}}{\mathrm{TSS}} = 1 - \frac{\sum_{i=1}^N\hat{\epsilon}_i^2}{\sum_{i=1}^N (Y_i-\bar{Y})^2}\]</span> R-square is also called <strong>coefficient ofdetermination</strong>.</p></blockquote><p>从上面这个表达式，我们可以从几何的角度大概理解了一下，<span class="math inline">\(R^2\)</span>是衡量个什么。首先，我们可以看到<span class="math inline">\(\hat{\epsilon}\)</span>表达的意思，就是<span class="math inline">\(N\)</span>维空间里的一个点<span class="math inline">\(Y\)</span>到<span class="math inline">\(\{X_{:,0},X_{:,1},X_{:,2},\cdots,X_{:,K}\}\)</span>构成的空间最近的欧式距离。那么很明显，我们可以得到关于<span class="math inline">\(R^2\)</span>的几个结论: - <span class="math inline">\(R^2\)</span>随着<span class="math inline">\(K\)</span>的增大而增大 - <span class="math inline">\(K\geN-1\)</span>，并且在满足<strong>Assumption5</strong>的时候，<span class="math inline">\(R^2=1\)</span> 因此，在使用<span class="math inline">\(R^2\)</span>去评价一个因子对目标的贡献的时候，永远都会得到一个非负的结果，而这个结果可能是由于过拟合导致的，并不是该因子一定做出来真正意义上的贡献了。</p><p>为了解决上面的这个问题，有人提出了一个调整后的<span class="math inline">\(R^2\)</span>，称之为<span class="math inline">\(R_{adj}^2\)</span> &gt; <strong>AdjustedR-square</strong> &gt; <span class="math display">\[R_{adj}^2 := 1 - \frac{\mathrm{RSS}/(N-K-1)}{\mathrm{TSS}/(N-1)}= 1 -(1-R^2)\frac{N-1}{N-K-1}&gt; \]</span></p><p>这边其实借鉴了F-test的思想。<span class="math inline">\(\mathrm{RSS}\)</span>其实有<span class="math inline">\(N-(K+1)\)</span>个“随机自由度”(<span class="math inline">\(K+1\)</span>个维度被<span class="math inline">\(X\)</span>所解释了)。<span class="math inline">\(\mathrm{TSS}\)</span>有<span class="math inline">\(N-1\)</span>个“随机自由度”(<span class="math inline">\(\bar{Y}\)</span>约束了其中一个维度)。</p><p>最后我们考虑一下<span class="math inline">\(R^2,R^2_{adj}\)</span>等于0的情况，那就是我们不采取任何其他的因子去解释<span class="math inline">\(Y\)</span>，永远就用样本的平均<span class="math inline">\(\bar{Y}\)</span>去作为<span class="math inline">\(Y\)</span>的估计值，这个时候，<span class="math inline">\(\mathrm{RSS}=\mathrm{TSS},K=1\)</span>。</p><h4 id="aic-bic">AIC &amp;&amp; BIC</h4><p><span class="math inline">\(R^2_{adj}\)</span>对于<span class="math inline">\(R^2\)</span>的调整，就是期望我们在最小化<span class="math inline">\(\mathrm{RSS}\)</span>的过程中，对于因子的个数有一定的惩罚。这个和现在机器学习里正则化的思想我认为也是大同小异。那么<span class="math inline">\(\mathrm{AIC}\)</span>和<span class="math inline">\(\mathrm{BIC}\)</span>两种<code>information criterion</code>则是将这个约束变得更加有数学意义点。&gt; <strong>AIC and BIC</strong> &gt; <span class="math display">\[&gt; \mathrm{AIC} = N + N\log2\pi + N \log \frac{\mathrm{RSS}}{N} +2(K+1), \\&gt; \mathrm{BIC} = N + N\log 2\pi + N \log \frac{\mathrm{RSS}}{N} +(K+1)\log N&gt; \]</span></p><h4 id="out-of-sample-rmse">Out-of-Sample RMSE</h4><p>最后这种，则是目前机器学习中，相对主流的方法，对于一个回归问题，使用样本外的均方差来评价模型的好坏。</p><h2 id="rls">RLS</h2><p>在考虑完简单的OLS之后，我们考虑一个跟实际更加接近的模型，即在现实生活中，我们真实的<span class="math inline">\(\beta\)</span>往往不是可以在空间里任意选择，而是仅仅只会位于某些更加低维的空间里面，即我们需要解决下面的问题,<span class="math display">\[Y = X\beta + \epsilon, \\L\beta = r, L \in \mathbb{R}^{M\times {K=1}}, r \in \mathbb{R}^{M\times1}\]</span> 当然这边<span class="math inline">\(M &lt;K+1\)</span>,否则就没有自由度供我们去优化<span class="math inline">\(\hat{\beta}\)</span>了。对于RLS的解，我们借助拉格朗日方法可以求解得到，细节见附录 &gt;<strong>RLS estimator</strong> &gt; <span class="math display">\[&gt; \hat{\beta}^{RLS} = \hat{\beta}^{OLS}-(X^TX)^{-1}L^T\left(L(X^TX)^{-1}L^T\right)^{-1}(L\hat{\beta}^{OLS}-r)&gt; \]</span></p><h3 id="衡量rls">衡量RLS</h3><p>通过对构造出来的优化问题的求解，我们得到了<span class="math inline">\(\hat{\beta}^{RLS}\)</span>，那么我也需要去从各个方面去评价其的好坏。首先就是，我们需要证明的<span class="math inline">\(\hat{\beta}^{RLS}\)</span>需要满足给定的限制条件<span class="math inline">\(L\hat{\beta}^{RLS} =r\)</span>，这点从其表达可以直接看出来。 再之，我们可以检查一下<span class="math inline">\(\hat{\beta}^{RLS}\)</span>作为一个估计算子，是否是无偏的并且一致的。对于偏置，我们计算一下期望<span class="math inline">\(\mathbb{E}[\hat{\beta}^{RLS}]\)</span>. 因为<span class="math inline">\(\mathbb{E}[\hat{\beta}^{OLS}] =\beta\)</span>(Assumption2), 我们很容易得出来，<span class="math inline">\(\hat{\beta}^{RLS}\)</span>也是一个无偏估计。对于一致性，因为<span class="math inline">\(\mathbb{Var}[\hat{\beta}^{OLS}] = \sigma^2(X^TX)^{-1}\)</span>, 当<span class="math inline">\(N\to\infty\)</span>,<span class="math inline">\(\mathbb{Var}[\hat{\beta}^{OLS}] \to0\)</span>, 也就是<span class="math inline">\(\hat{\beta}^{OLS} \to\beta\)</span>, 那么我们也很容易发现<span class="math inline">\(\hat{\beta}^{RLS} \to\beta\)</span>。总结一下,</p><p>那么<span class="math inline">\(\hat{\beta}^{RLS}\)</span>的效率如何呢，在对<span class="math inline">\(\beta\)</span>有限制的前提下，<span class="math inline">\(\hat{\beta}^{RLS}\)</span>会不会比<span class="math inline">\(\hat{\beta}^{OLS}\)</span>估计更加的高效呢，那么只需要两个估计算的协方差<span class="math inline">\(\mathbb{Var}[\hat{\beta}^{RLS}]\)</span>和<span class="math inline">\(\mathbb{Var}[\hat{\beta}^{OLS}]\)</span>,通过一些简单的计算，我们可以发现, <span class="math display">\[\mathbb{Var}[\hat{\beta}^{OLS}] - \mathbb{Var}[\hat{\beta}^{RLS}] =\sigma^2(X^TX)^{-1}L^T\left(L(X^TX)^{-1}L^T\right)^{-1}L(X^TX)^{-1} \ge0\]</span></p><blockquote><p><strong>RLS Summary</strong> If the restrictions are correct, <span class="math inline">\(L\beta = r\)</span>, and all assumptions satisfiesthen: - <span class="math inline">\(\hat{\beta}^{RLS}\)</span> isunbiased - <span class="math inline">\(\hat{\beta}^{RLS}\)</span> ismore efficient than <span class="math inline">\(\hat{\beta}^{OLS}\)</span> - <span class="math inline">\(\hat{\beta}^{RLS}\)</span> is BLUE <span class="math inline">\(\hat{\beta}^{RLS}\)</span> is consistent <span class="math inline">\(\hat{\beta}^{RLS}\)</span> is asymptoticallynormally distributed: <span class="math display">\[\hat{\beta}^{RLS} \sim \mathcal{N}\left(\beta,\sigma^2D(X^TX)^{-1}\right),\]</span> where <span class="math inline">\(D=I-(X^TX)^{-1}L^T\left(L(X^TX)^{-1}L^T\right)^{-1}L\)</span></p><p>If the restrictions are wrong, <span class="math inline">\(L\beta =r\)</span>, then: <span class="math inline">\(\hat{\beta}^{RLS}\)</span>is biased(no longer BLUE) <span class="math inline">\(\hat{\beta}^{RLS}\)</span> is remains moreefficient than <span class="math inline">\(\hat{\beta}^{OLS}\)</span><span class="math inline">\(\hat{\beta}^{RLS}\)</span> isinconsistent</p></blockquote><p>这边我们需要说明的，<span class="math inline">\(\hat{\beta}^{RLS}\)</span>的方差一直比<span class="math inline">\(\hat{\beta}^{OLS}\)</span>要小，无论限制条件是否满足，但是这并不影响<span class="math inline">\(\hat{\beta}^{OLS}\)</span>是<code>BLUE</code>在没有限制条件的下，因为一旦限制条件被破坏，<span class="math inline">\(\hat{\beta}^{RLS}\)</span>虽然效率更加高，但是它不再是一个无偏的估计算子了。</p><p>既然先验的条件，对于一个估计算子来说是异常之重要，那么我们可以采用传统的假说检验，来帮助我们验证先验的正确性。<span class="math display">\[H_0: L\beta = r, H_1 : L\beta \neq r\]</span> 采用F-test去做假说检验的话，只需要计算， <span class="math display">\[F := \frac{(\mathrm{RSS}_R -\mathrm{RSS}_{UR})/M}{\mathrm{RSS}_{UR}/(N-(k+1))}\]</span></p><h2 id="共线性">共线性</h2><p>上面两个问题，无论是OLS还是RLS，要想求解都离不开<strong>Assumption5</strong>，也就是说我们需要<span class="math inline">\((X^TX)\)</span>这个矩阵是可逆的，或者就是我们给出因子之间不要存在线性关联。但是在实际建模中，尤其是面对可解释性没有那么强的因子时，这个条件往往不成立，或者说我们很难准确地判断这个条件是不是成立。因此在实际中使用的时候，因此数据或多或少存在一些噪声，求解器往往不会直接抛出<span class="math inline">\((X^TX)\)</span>不可逆的错误，但是我们并不能忽略共线性带来的影响。</p><p>我们很容想到的问题就有：</p><ul><li>估计算子的方差<span class="math inline">\(\sigma^2(X^TX)^{-1}\)</span>会很大，很不稳定</li><li>在做预测的时候，因子的微笑变化，甚至该变化是来源自噪声，也可能导致<span class="math inline">\(Y\)</span>的巨大差异</li><li><span class="math inline">\(R^2\)</span>远超预期的大，（这点我觉得因素太多，未必是共线性导致的）</li><li>过拟合，因为共线性的因子包含的信息其实就是那么多，但是因子个数却在增加，因此很容易造成过拟合，这点和<span class="math inline">\(R^2\)</span>超预期也是类似</li></ul><p>那么如何去检验我们因子中是否存在严重的共线性，则是在做线性回归的时候，一个前置问题。一般可能会有两种方法</p><h3 id="variance-inflation-factorvif">Variance InflationFactor(VIF)</h3><p>VIF的想法则是十分简单的，如果被检测的因子<span class="math inline">\(X_{:,j}\)</span>是一个连续变量，那么我们直接用剩余的因子对其做一个线性回归，并且计算其对应的<span class="math inline">\(R^2_j\)</span>, 如果<span class="math inline">\(R_j^2\)</span>过大，则认为该因子和其他因子存在比较严重的共线性。<span class="math display">\[X_{:,j} = \sum_{k\neq j}X_{:,k}\alpha_k + e_j\]</span>当然这边为了和之后定义的<code>GVIF</code>有个对应的关系，一般检测的是下面这个值&gt; <strong>VIF</strong> &gt; <span class="math display">\[&gt; \mathrm{VIF}_j := \frac{1}{1-R_j^2}&gt; \]</span></p><h3 id="generalized-variance-inflation-factorgvif">Generalized VarianceInflation Factor(GVIF)</h3><p>VIF要求我们构建出一个线性回归问题，但是在有些时候线性回归问题并不是可以构造的，例如被检测因子和其他因子存在多项式关系，或者被检测因子是一个类别性质的因子。那么这个时候我们需要将被检测的因子归入到一个集合内，比如，- 多项因子：<span class="math inline">\(\mathcal{F}=(x_1, x_1^2, x_1^3,\cdots)\)</span> - 类别哑变量：<span class="math inline">\(\mathcal{F}=(1_{x\in C_1}, 1_{x\in C_2},\cdots)\)</span></p><p>这个时候我们的因子被分成两个集合<span class="math inline">\(\mathcal{F}, \mathcal{F}_C\)</span>,那么我们可以计算他们的correlation matrix。 &gt; <strong>GVIF</strong>&gt; <span class="math display">\[&gt; \mathrm{GVIF} := \frac{\det \mathbb{Cov}(\mathcal{F}) \det\mathbb{Cov}(\mathcal{F}_C)}{\det \mathbb{Cov}(\mathcal{F} \cup\mathcal{F}_C)}&gt; \]</span></p><h3 id="处理共线性">处理共线性</h3><p>处理共线性的方法：</p><ul><li>警惕<em>哑变量陷阱</em>，即当你对类别因子采用哑变量的时候，如果这个时候还有一个偏置项，那么偏置和哑变量则必然构成共线性。</li><li>通过上述检测方法，剔除共线性强烈的因子，但是如果目标不是做因子检测，而是单纯的想去预测<span class="math inline">\(Y\)</span>,剔除并不是一个很好的方法，因为我们会丢失对<span class="math inline">\(Y\)</span>的信息。这个时候，最好的方法去寻找规避过拟合的方法</li><li>防止过拟合<ul><li>增加正则项</li><li>获取更多的数据，缓解过拟合和共线性带来的影响</li><li>有限bound的因子，选择合适的normarlize技术</li></ul></li></ul><h2 id="gls">GLS</h2><p>在共线性部分，我们考虑了relax <strong>Assumption5</strong>的各个方面。那么在<code>GLS</code>这个部分，我们考虑的是relax<strong>Assumption 3-4</strong>。<strong>Assumption3-4</strong>要求的是对于随机变量<span class="math inline">\(\epsilon\)</span>,它们之间的协方差矩阵是一个正比于单位矩阵的量，也是保证各个error之间是相互无关而且各向同性。那么当然，这边抹除这个假设，则是我们认为协方差可以写成一个通用的形式，<span class="math display">\[\mathbb{Cov}(\epsilon, \epsilon) = \sigma^2 \Omega\]</span> 这边<span class="math inline">\(\Omega\)</span>是一个半正定矩阵。</p><h3 id="ols表现">OLS表现</h3><p>我们可以先看一下<span class="math inline">\(\hat{\beta}^{OLS}\)</span>在这个场景下面的表现。首先因为<strong>Assumption2</strong>的成立，因此<span class="math inline">\(\hat{\beta}^{OLS}\)</span>依然是无偏的，而且很容易计算，<span class="math display">\[\mathbb{Cov}(\hat{\beta}^{OLS}, \hat{\beta}^{OLS}) = \sigma^2(X^TX)^{-1}X^T\Omega T (X^TX)^{-1} \neq \sigma^2 (X^TX)^{-1}\]</span> 从上面的表示，我们依然可以看到<span class="math inline">\(N\to\infty, \mathbb{Cov}(\hat{\beta}^{OLS},\hat{\beta}^{OLS}) \to 0\)</span>, 因此，在这种情况下面， <strong><span class="math inline">\(\hat{\beta}^{OLS}\)</span>仍然是一个无偏并且一致的估计算子</strong></p><p>但是我们上面的统计推断的很多方法在这边都不再使用了，因为在没有关联并且各向同性的误差假设，我们采用了<span class="math inline">\(\hat{\epsilon}^T\hat{\epsilon}\)</span>来估计<span class="math inline">\(\hat{\sigma}^2\)</span>，那么显然在这边<span class="math inline">\(\hat{\epsilon}^T\hat{\epsilon}=\frac{1}{N-(K+1)}((Y-X\hat{\beta}^{OLS})^T(Y-X\hat{\beta}^{OLS}))\)</span>是一个有偏并且不一致的估计了。</p><h3 id="通用的估计算子">通用的估计算子</h3><p>基本线性代数方法，告诉我们对于相关的随机变量的协方差矩阵，要对这些随机变量进行解耦，最简单的方法就是使用正交矩阵直接对角化这个协方差矩阵。然后为了保证各个方向的各向同性，我们再在新的坐标系下进行一些简单的伸缩变换就可以了。综合上面两步，我们可以找到一个三角矩阵<span class="math inline">\(U\)</span>，满足 <span class="math display">\[\Omega^{-1} = UU^T\]</span>那么很容易我们可以将通过下面的变换<code>GLS</code>转化成<code>OLS</code><span class="math display">\[\tilde{Y} = \tilde{X} \beta + \tilde{\epsilon}, \tilde{Y}:=U^TY,\tilde{X}:=U^TX, \tilde{\epsilon}:=U^T \epsilon\]</span> 因此 &gt; <strong>GLS Estimator</strong> &gt; <span class="math display">\[&gt; \hat{\beta}^{GLS} = (X^TUU^TX)^{-1}X^TUU^TY =(X^T\Omega^{-1}X)^{-1}X^T\Omega^{-1}Y&gt; \]</span></p><h3 id="summary">Summary</h3><p>通过<span class="math inline">\(U\)</span>变换之后，<code>OLS</code>的那些分析都可以拓展到<code>GLS</code>中来，这边只是总结一下最终的结果，具体可自行根据上面的内容推导.</p><blockquote><p><strong>GLS Summary</strong> - The error variance <span class="math display">\[\hat{\sigma}^2=\frac{1}{N-(K+1)}(\tilde{Y}-\tilde{X}\hat{\beta}^{GLS})^T(\tilde{Y}-\tilde{X}\hat{\beta}^{GLS})\\ =\frac{1}{N-(K+1)}((Y-X\hat{\beta}^{GLS})^T)\Omega^{-1}(Y-X\hat{\beta}^{GLS})\]</span><span class="math inline">\(\hat{\sigma}^2\)</span>是对<span class="math inline">\(\sigma^2\)</span>的一个无偏，一致的估计。 - <span class="math inline">\(\hat{\beta}^{GLS}\)</span> 是一个无偏的估计, <span class="math inline">\(\mathbb{E}[\hat{\beta}^{GLS}] = \beta\)</span> -协方差矩阵为<span class="math inline">\(\mathbb{Cov}(\hat{\beta}^{GLS},\hat{\beta}^{GLS}) = \sigma^2(X^T\Omega^{-1}X)^{-1}\)</span> -如果<strong>Assumption5</strong>成立，那么<span class="math inline">\(\hat{\beta}^{GLS} | X \sim \mathcal{N}(\beta,\sigma^2(X^T\Omega^{-1}X)^{-1})\)</span> - <span class="math inline">\(\hat{\beta}^{GLS}\)</span> is BLUE</p></blockquote><h3 id="现实与理想">现实与理想</h3><p>我们可以发现上面的所有关系，从线性变换开始，都隐含假设了我们知道了<span class="math inline">\(\Omega\)</span>这个矩阵。当然在现实中，我们有的情况是知道<span class="math inline">\(\Omega\)</span>的，比如Barra纯因子模型中，假设了波动和股票市值的平方根成正比，从而认为的构造出了一个<span class="math inline">\(\Omega\)</span>。但是在绝大部分数学分析的工作中，<span class="math inline">\(\Omega\)</span>都是未知的。因此如何去构造一个<span class="math inline">\(\Omega\)</span>的估计<span class="math inline">\(\hat{\Omega}\)</span>才是整个建模的关键。 <!-- 在这节我们同时打破了假设**Assumption3**和**Assumption4**直接造成了现实与理想如此巨大的鸿沟。接下来两节，我们将摒弃这季度追求普适性的做法，分别分析在打破这两个假设其中之一的情况。## 各向同性在这节中，**Assumption3**被打破，**Assumption4**仍然成立。 --></p><h2 id="附录">附录</h2><h3 id="gauss-markov-theorem">Gauss-Markov Theorem</h3><p>任何一个线性的estimator <span class="math inline">\(\tilde{\beta} = MY\)</span>, 我们需要证明的 <span class="math inline">\(\hat{\beta}\)</span>在所有的<span class="math inline">\(\tilde{\beta}\)</span>中拥有最小的估计方差，以及其能保证一致性。对于一致性来说，我们需要证明<span class="math inline">\(\mathbb{E}[\hat{\beta}]=\beta\)</span>. <span class="math display">\[\mathbb{E}[\hat{\beta}] = \mathbb{E}_X\left[\beta |X +(X^TX)^{-1}X^T\mathbb{E}[\epsilon | X]\right] = \beta\]</span> 当然从上面的式子，我们也可以看出来 <span class="math display">\[\mathbb{E}[\hat{\beta}|X] = \beta\]</span> 这边我们只用到了<strong>Assumption2</strong></p><p>下面我们开始证明，OLS具有最小的条件协方差矩阵，即 <span class="math inline">\(\mathbb{Cov}(\hat{\beta}, \hat{\beta} | X) \le\mathbb{Cov}(\tilde{\beta}, \tilde{\beta} | X)\)</span>. 令<span class="math inline">\(M = D + (X^TX)^{-1}X^T\)</span>,我们很容易计算出来， <span class="math display">\[\mathbb{Cov}(\tilde{\beta}, \tilde{\beta} | X ) = DYY^TD^T +\mathbb{Cov}{(\hat{\beta}, \hat{\beta} | X)}\]</span> QED</p><h3 id="rls问题">RLS问题</h3><p>对于RLS的拉格朗日量可以写成， <span class="math display">\[\mathcal{L}(\beta, \lambda) = (Y-X\beta)^T(Y-X\beta) + 2\lambda^T(L\beta-r)\]</span> 其对应的导数为 <span class="math display">\[\frac{\partial \mathcal{L}}{\partial \beta} = -2X^TY + 2X^TX\beta +2L^T\lambda = 0 \\\frac{\partial \mathcal{L}}{\partial \lambda} = 2L\beta - 2r = 0\]</span> 那么<span class="math inline">\(\beta,\lambda\)</span>则是下面线性方程组的解, <span class="math display">\[\left(\begin{array}{cc}X^TX &amp; L^T \\ L &amp; 0\end{array}\right)\left(\begin{array}{c}\beta \\ \lambda\end{array}\right) =\left(\begin{array}{c}X^TY \\ r\end{array}\right)\]</span> 通过求逆，我们很容易得到 $$ ^{RLS} = ^{OLS}-(X<sup>TX)</sup>{-1}L<sup>T(L(X</sup>TX)<sup>{-1}L</sup>T)<sup>{-1}(L</sup>{OLS}-r),\ = (L(X<sup>TX)</sup>{-1}L<sup>T)</sup>{-1}(L^{OLS} - r) $</p><h2 id="参考文献">参考文献</h2><ol type="1"><li>Book, Element of statistical learning</li><li>https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1601414</li><li>http://web.vu.lt/mif/a.buteikis/wp-content/uploads/PE_Book/</li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Time Series Analysis</title>
      <link href="/2022/07/25/time_series/"/>
      <url>/2022/07/25/time_series/</url>
      
        <content type="html"><![CDATA[<h1 id="Time-Series-Analysis"><a href="#Time-Series-Analysis" class="headerlink" title="Time Series Analysis"></a>Time Series Analysis</h1><p>This is unfinised time series analysis note, which is wished to be done around 2023 March<br><a id="more"></a></p><h2 id="ARMA"><a href="#ARMA" class="headerlink" title="ARMA"></a>ARMA</h2><blockquote><p>Definition[White Noise]<br>We call a sequence of r.v ${e_t}$ as white noise, if $\mathrm{E}[e_t]=0, \mathrm{E}[e_t^2] = \sigma^2, \mathrm{E}[e_te_s] = 0, \forall t \neq s$</p><p>Definition[strictly stationary]<br>A process ${y<em>t}$ is called strictly stationary if for each $t,k,n$ $y_t, \cdots, y</em>{t+k}}$ has the same distribution as ${y<em>{t+n}, \cdots, y</em>{t+n+k}}$</p><p>Definition[weakly stationary]<br>A process ${y<em>t}$ is called weakly stationary if for each $k,t$ $\mathrm{E}[y_t], \mathrm{E}[y_t^2], \mathrm{E}[y_ty</em>{t+k}]$ is independent of $t$</p><p>Definition[Lag Operator]<br>Lag operator is defined as $Ly<em>t = y</em>{t-1}$</p><p>Corollary</p><ul><li>Lag polynomial defined as $a(L;p):=\sum_{i=0}^{p}a_i L^i$ has commuted multiplication, i.e. $a(L; p)b(L; q) = b(L; q)a(L; p)$</li><li>If the roots of $a(L)$ are out of unit circle, then it is invertible.</li></ul></blockquote><p>Here, we demonstrate a simple example of the second point. For a polynomial $a(L; 1) = 1 - \lambda L $, whose root is out of unit circle when $|\lambda| &lt; 1$, its inverse can be represented as,</p><script type="math/tex; mode=display">a^{-1}(L,1) = \sum_{i=0}^{+\infty}\lambda^i L^i</script><h3 id="Simple-Processes"><a href="#Simple-Processes" class="headerlink" title="Simple Processes"></a>Simple Processes</h3><blockquote><p>Definition[<strong>AR</strong>]<br>Auto regressive process with order $p$ is defined as </p><script type="math/tex; mode=display">a(L;p)y_t = e_t</script><p>Definition[<strong>MA</strong>]<br>Moving Averaging Process with order $q$ is defined as </p><script type="math/tex; mode=display">y_t = b(L;q)e_t</script><p>Definition[<strong>ARMA</strong>]<br>The $(p,q)$ order of ARMA is,</p><script type="math/tex; mode=display">a(L;q)y_t = b(L;q)e_t</script></blockquote><p><strong>AR</strong> is regression, since we can expand the definition as $a<em>0 y_t = \sum</em>{i=1}^p(-a<em>i)y</em>{t-i} + e<em>t$, which is just we do regression of $y_t$ in terms of $y</em>{t-1}, \cdots, y_{t-i}$</p><p>The intution of <strong>MA</strong> is more direct, that is,</p><script type="math/tex; mode=display">y_t = \sum_{i=0}^q b_i e_{t-i}</script><p>we use weights ${b_i}$ to average another stochastic process.</p><p>Hence, <strong>ARMA</strong> just tries to find a linear combination of signal sequences $y_t$ to fit the linear of combination of noise sequence $e_t$.</p><blockquote><p>Corollary<br>Combing with the above invertible conditions, we claim that there exists a bridge between <strong>AR</strong> and <strong>MA</strong>.</p></blockquote><p>For example, <strong>AR</strong>$(p)$ has an equivalent representation of <strong>MA</strong>$(\infty)$ if $a(L;p)$ is invertible. The reversed claim for <strong>MA</strong>$(q)$ and <strong>AR</strong>$(\infty)$ is also correct.</p><h3 id="Covariance"><a href="#Covariance" class="headerlink" title="Covariance"></a>Covariance</h3><blockquote><p>Definition[auto-covariance]<br>$\gamma<em>k := \mathrm{Cov}(y_t, y</em>{t+k})$ for weakly stationary process</p><p>Definition[auto-correlation]<br>$\rho_k : = \frac{\gamma_k}{\gamma_0}$ for weakly stationary process</p><p>Definition[covariance function]<br>$\tilde{\gamma}(\xi) := \sum_{i=-\infty}^{+\infty}\gamma_i \xi^i$</p><p>Corollary[<strong>ARMA</strong> covariance function]<br>The covariance function of <strong>ARMA</strong> is</p><script type="math/tex; mode=display">\tilde{\gamma}(\xi) = \frac{b(\xi)b(\xi^{-1})}{a(\xi)a(\xi^{-1})} \sigma^2</script></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Diffusion Map Method</title>
      <link href="/2022/07/03/diffusion_map/"/>
      <url>/2022/07/03/diffusion_map/</url>
      
        <content type="html"><![CDATA[<h1 id="diffusion-map-method">Diffusion map method</h1><p>本篇介绍一种半监督的算法，以及其在理论物理里面的应用 <a id="more"></a></p><h2 id="引言">引言</h2><p>这篇的Blog的源于，最近读到了一篇<strong>PRL</strong>的paper《Unsupervisedmachine learning of quantum phase transitions using diffusion maps》(https://arxiv.org/pdf/2003.07399.pdf)。这篇文章使用了<code>DMM</code>（diffusion mapmethod）来分析量子相变的一些特性，还是蛮有意思的，我会在这边Blog介绍完<code>DMM</code>方法之后，大概说一下其中的一些想法。</p><p>在打开这篇paper之前，在我陈旧的机器学习知识框架里，确实没有<code>DMM</code>这个东西，然后我就去好奇去Google了一下，大概发现了两篇有用的paper。</p><ul><li>第一篇是2005年<strong>PNAS</strong>（https://www.pnas.org/doi/epdf/10.1073/pnas.0500334102)，由Yale几位科学家提出来的，将<code>DMM</code>用来做调和分析的工具。</li><li>第二篇(https://inside.mines.edu/~whereman/papers/delaPorte-Herbst-Hereman-vanderWalt-PRASA-2008.pdf)是一篇不错的tutorial性质的文章。</li></ul><p>读完这些，<code>DMM</code>这个方法让我惊艳的地方，一句话大概是：</p><blockquote><p>NOTE 从局部性质衍生全局性质，进而进行分析的一种算法</p></blockquote><p>正如第一篇PNAS中所说：</p><p><em>The process of iterating or diffusing the Markov matrix is seenas a generalization of some aspects of the Newtonian paradigm, in whichlocal infinitesimal transitions of a systemlead to global macroscopicdescriptions by integration.</em></p><h2 id="dmmdifussion-map-method">DMM(Difussion map method)</h2><p>算法<code>DMM</code>大概用两种用途：</p><ul><li>数据降维</li><li>聚类</li></ul><p>当然一个算法如果能很好地对数据进行降维，额外做数据的聚类也是一个简单的任务，比如结合K-Means等等这种常规算法。下面我们就主要介绍降维方面的一些想法</p><h3 id="现有的算法">现有的算法</h3><h4 id="pca">PCA</h4><p>提到数据降维，我想作为炼丹师，一下能够想到很多出名的算法，其中不得不说，非<code>PCA</code>(principalcomponentanalysis)莫属。当然<code>PCA</code>是一个线性算法。这可以从几方面来理解。- 从代数的角度，<code>PCA</code>算法用到的操作都是一个线性空间的线性操作 -从几何的角度，<code>PCA</code>只是通过旋转坐标轴的，找到那些数据高区分度的坐标轴进行保留。</p><p>这其中最关键的，所谓的在线性空间进行线性操作，无论怎么搞，坐标轴永远是“直”的。但是我们知道，比如我空间里一条曲线，在“直”的坐标系下面仍然需要三个坐标去描述，但是实际上，一条曲线的有效的维度永远是一维的（当然这边描述不够严谨，比如也是有一些奇怪的曲线维度是分数维的，这边只是对于那种普通的曲线，供大家理解）。例如，下面这条曲线上，我用三种颜色表示了三个聚类，在这种数据下，我们是没有办法使用PCA去做数据降维或者聚类分析。<img src="/2022/07/03/diffusion_map/original.png" class="" title="This is an image"></p><h4 id="mdsmutidimensional-scaling">MDS(Mutidimensional Scaling)</h4><p>算法<code>MDS</code>的idea，我觉得是非常棒的，比<code>PCA</code>更加自然，也是如果一个没有学过机器学习的人，让他来做数据降维，最容易想到的一个做法。这边我们简单描述这个想法：</p><blockquote><p>假设我们有个<span class="math inline">\(N\)</span>个数据点<span class="math inline">\(x_i\)</span>，我们需要找一个map，将每个<span class="math inline">\(x_i\)</span>映射到<span class="math inline">\(y_i\)</span>，还尽可能保留数据之间的"关系"。比如找到一种数据描述<span class="math inline">\(d_X,d_Y\)</span>来建模这种"关系"。那么对于任意<span class="math inline">\(d_X(x_i,x_j)\)</span>， 我们希望<span class="math inline">\(d_X(x_i,x_j)\)</span>小(大)的，映射之后对应的关系<span class="math inline">\(d_Y(y_i,y_j)\)</span>也小(大)。</p></blockquote><p>当然<code>MDS</code>采用了欧式距离来建模这种“关系”，这也是这个方法的局限，毕竟欧式距离做为线性结构的度量是不错的，但是如果数据之间的关系是非线性的，那么欧式距离只是在邻域内看作是一个相对有效的表达。</p><h4 id="dmm">DMM</h4><p>那么在我们不知道数据的几何结构的时候（如果知道，只需要拟合即可，问题就会简单了），我们如果才能如何有效地表达出<span class="math inline">\(d_X,d_Y\)</span>呢。<code>DMM</code>提出了一个核心想法： &gt;通过扩散过程，衔接局部性质和全局性质</p><p>这种想法在物理研究其实很常见：</p><ul><li>牛顿力学或是量子力学都是通过建立微分方程(局部)，然后积分求解，得到一些宏观的可观测量(全局)</li><li>蒙特卡洛模拟，也是使用局部的概率进行模拟，通过Hamiltonian带来的相互作用，将整个影响传播至全局，最后得一个平衡态。</li><li>等等</li></ul><p>在<code>DMM</code>算法中通过一个核函数<span class="math inline">\(k(x_i,x_j)\)</span>来模拟原始数据之间的局部关系，比如常用的高斯核<span class="math inline">\(k(x_i,x_j) =e^{-\frac{\|x_i-x_j\|^2}{2\epsilon}}\)</span>。拥有了核函数对应的矩阵<span class="math inline">\(K_{ij}:=k(x_i,x_j)\)</span>，我们可以对每一行进行归一化，得到一个符合转移矩阵的要求矩阵<span class="math inline">\(P:=D^{-1}K\)</span>，这边<span class="math inline">\(D\)</span>是一个对角矩阵，每个元素是对应行的<span class="math inline">\(K\)</span>矩阵的元素之和。那么这个扩散过程，我们就可以通过<span class="math inline">\(P, P^2, P^3, \cdots, P^t,\cdots\)</span>来模拟。假设我们模拟了<span class="math inline">\(t\)</span>步，概率流在这个图中流淌了一段时间，局部的性质通过这种流淌，慢慢全局化。那么这时候需要一个问题，在这种建模的情况，我们如何去描述第<span class="math inline">\(i\)</span>和<span class="math inline">\(j\)</span>数据之间的关系呢？回答好这个问题，那么<code>DMM</code>的算法就完毕了，剩余的只是一些不那么关键的数学推导了。在当前的建模情况，<span class="math inline">\(P\)</span>矩阵的第<span class="math inline">\(i\)</span>行表示的第<span class="math inline">\(i\)</span>组数据和其他数据之间的转移概率。那么描述<span class="math inline">\(i\)</span>和<span class="math inline">\(j\)</span>的相似度，其实就变得很简单了，就是描述两个概率分布之间的相似度。最常见的两种就是：</p><ul><li>L2 <span class="math inline">\(\sum_k|P^t_{ik}-P^t_{jk}|^2\)</span></li><li>CrossEntropy <span class="math inline">\(-\sum_k P^t_{ik}\logP^t_{jk}\)</span></li></ul><p>当然在<code>DMM</code>算法中他选择了第一种描述关系，这是为了他做数据降维方便而做的选择。正如我们之前说的L2是一个描述线性结构的度量方式，选择了<span class="math inline">\(L2\)</span>就是采用数据经过扩散过程之后，局部的关系变成了全局的关系，并且全局的关系进入了一个相对平坦的子流形上去了。</p><blockquote><p>这边我自己也很好奇，如果我选用第二种相似度描述方式，该如何去做数据降温呢？</p></blockquote><p>到了这一步，我们拥有了一个数据的有效表示<span class="math inline">\(P^t_{i*}\)</span>，同时还假设了线性，那么这个时候使用什么方式去做数据降维那么就是显而易见了，我们只需要对<span class="math inline">\(P^t\)</span>做一次<code>PCA</code>，就可以做到了。当然了在原始的paper，研究人员使用了一些数学手段简化了这个<code>PCA</code>过程，有兴趣的可以看看附录吧。</p><p>到了这边，有了数据表示和数据相似度的数学建模，那么除了数据降维，去做一个聚类任务也是比较简单的事情了。比如对上面的那组测试数据，我们使用<code>DMM</code>降三维数据降到二维，得到下面的结果<img src="/2022/07/03/diffusion_map/two_dim.png" class="" title="This is an image"> 我们很容易得出下面两点结论：</p><ul><li>数据的order得到了保证，蓝-橘-绿</li><li>数据变得更加的紧致</li></ul><p>如果降维到一维的结果，就是把上面这个图向<span class="math inline">\(x\)</span>轴投影，我们会发现上面的两点结论仍然是成立的。这个Demo的例子可以用下面这段生成</p><p></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">from mpl_toolkits.mplot3d import Axes3D</span><br><span class="line"></span><br><span class="line">def gaussian_kernel(point1, point2, alpha: float = 0.01):</span><br><span class="line">    return np.exp(-np.linalg.norm(point1 - point2)**2 / alpha)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class DiffusionMap:</span><br><span class="line">    """</span><br><span class="line">    a simple realization of diffusion map method</span><br><span class="line">    """</span><br><span class="line">    def __init__(self):</span><br><span class="line">        pass</span><br><span class="line"></span><br><span class="line">    @staticmethod</span><br><span class="line">    def diff_map(data: np.ndarray, kernel_func, keep_dim: int = 2):</span><br><span class="line">        """</span><br><span class="line">        Params:</span><br><span class="line">        -------------------</span><br><span class="line">        data: N x M numpy array, N data points with original dimension M</span><br><span class="line">        kernel_func: a function, give the measure between two data points</span><br><span class="line">        keep_dime: int, the number of dimension left</span><br><span class="line">        """</span><br><span class="line">        data_num = data.shape[0]</span><br><span class="line">        kernel_mat = np.zeros((data_num, data_num))</span><br><span class="line">        for i in range(data_num):</span><br><span class="line">            for j in range(data_num):</span><br><span class="line">                kernel_mat[i, j] = kernel_func(data[i, :], data[j, :])</span><br><span class="line">        normalize_mat = np.diag(np.sum(kernel_mat, axis=1))</span><br><span class="line">        normalize_mat_sqrt = np.sqrt(normalize_mat)</span><br><span class="line">        normalize_mat_sqrt_inv = np.linalg.inv(normalize_mat_sqrt)</span><br><span class="line">        transition_mat_sim = normalize_mat_sqrt_inv.dot(kernel_mat).dot(normalize_mat_sqrt_inv)</span><br><span class="line">        print("symmetry error:", np.sum(np.abs(transition_mat_sim - transition_mat_sim.T)))</span><br><span class="line">        # transition_mat = kernel_mat / np.sum(kernel_mat, axis=1, keepdims=True)</span><br><span class="line">        eig_vals, eig_vecs_sim = np.linalg.eig(transition_mat_sim)</span><br><span class="line">        # print("eig_vals:", eig_vals)</span><br><span class="line">        eig_vals_sort = np.sort(eig_vals)[::-1]</span><br><span class="line">        eig_vals_sort_index = np.argsort(eig_vals)[::-1]</span><br><span class="line">        eig_vecs_tran = normalize_mat_sqrt_inv.dot(eig_vecs_sim)</span><br><span class="line">        if keep_dim &gt; data_num - 1:</span><br><span class="line">            keep_dim = data_num - 1</span><br><span class="line">        diffusion_map = np.zeros((data_num, keep_dim))</span><br><span class="line">        for i in range(keep_dim):</span><br><span class="line">            val = eig_vals_sort[i+1]</span><br><span class="line">            eig_vec = eig_vecs_tran[:, eig_vals_sort_index[i+1]]</span><br><span class="line">            print("norm:", np.linalg.norm(eig_vec))</span><br><span class="line">            diffusion_map[:, i] = val * eig_vec</span><br><span class="line">        return diffusion_map</span><br><span class="line"></span><br><span class="line">if __name__ == "__main__":</span><br><span class="line">    n = 300</span><br><span class="line">    r = np.linspace(0, 1, n)</span><br><span class="line">    theta = np.linspace(0, 6 * np.pi, n)</span><br><span class="line">    phi = np.linspace(0, np.pi / 4, n)</span><br><span class="line">    x = r * np.cos(theta) * np.cos(phi)</span><br><span class="line">    y = r * np.sin(theta) * np.cos(phi)</span><br><span class="line">    z = r * np.sin(phi)</span><br><span class="line">    fig1 = plt.figure()</span><br><span class="line">    ax1 = Axes3D(fig1)</span><br><span class="line">    ax1.scatter(x[:100], y[:100], z[:100])</span><br><span class="line">    ax1.scatter(x[100:200], y[100:200], z[100:200])</span><br><span class="line">    ax1.scatter(x[200:], y[200:], z[200:])</span><br><span class="line">    data = np.concatenate([x.reshape(-1,1), y.reshape(-1,1), z.reshape(-1,1)], axis=1)</span><br><span class="line">    reduce_data = DiffusionMap.diff_map(data, gaussian_kernel)</span><br><span class="line">    fig2 = plt.figure()</span><br><span class="line">    plt.scatter(reduce_data[:100,0], reduce_data[:100,1])</span><br><span class="line">    plt.scatter(reduce_data[100:200,0], reduce_data[100:200,1])</span><br><span class="line">    plt.scatter(reduce_data[200:,0], reduce_data[200:,1])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="使用dmm研究量子相变">使用DMM研究量子相变</h2><p>这边主要是展示一下之前那篇<strong>PRL</strong>的其中之一的一个工作。在这边文章中，作者使用DMM来做聚类任务，认为类别的数目可以作为一个宏观的量来描述各种物理相。</p>首先，咱们先把物理抛在脑后，先思考一个问题<span class="math inline">\(\epsilon\)</span>和聚类的数目的关系应该是什么样子的。<img src="/2022/07/03/diffusion_map/prl1.png" class="" title="This is an image"><center>特征半径和聚类数目的关系</center><p>如上图中蓝线所示，我们可以想象，当<span class="math inline">\(\epsilon\)</span>很小的时候，核函数衰减得非常快，那么每个数据点，就很容易被隔离开，自己单独形成一个聚类。随着<span class="math inline">\(\epsilon\)</span>的慢慢增大，一些相近的数据点之间的连接逐渐形成，那么聚类开始成团簇状。直到<span class="math inline">\(\epsilon\)</span>变得很大，核函数在整个尺度都不怎么衰减，所有数据点构成一个团簇。显然，中间的这种状态是比较有意思的情况，两端的情况则相对trivial。</p><p>下面作者用了一个理论物理常用的模型， <span class="math inline">\(Z_3\)</span>的横场Ising模型，哈密顿量可以写成下面这种形式:<span class="math display">\[H = -f \sum_{j=1}^N \tau_j e^{i\theta} - (1 - f)\sum_{j=1}^N \sigma_j\sigma_{j+1}^{\dagger}e^{i\theta} + h.c.\]</span> 众所周知，这个模型： - 在没有手性的情况(<span class="math inline">\(\theta=0\)</span>)，有两个相，铁磁相和顺磁相。 -在有手性的情况(<span class="math inline">\(\theta &gt; 0\)</span>),在铁磁和顺磁之间，还有一个额外的相IC(incommensurate phase,我也不知道中文是啥)。这个中间相不同铁磁和顺磁那么好区分。因为它的自发磁化强度也是0，因此<span class="math inline">\(\sum_{j=1}^N \langle \sigma_j \rangle\)</span>作为一个宏观可观测量，不足以区分这两种相。</p><p>这边作者通过测量哈密顿量对应的基态对应的磁化强度，得到了一些数据点<span class="math inline">\(\vec{M}_i\)</span>，然后通过对这些数据点，使用<code>DMM</code>做聚类分析，画出了不同的参数<span class="math inline">\(f,\theta\)</span>下面，对应的聚类数目密度图，惊喜地发现（如图）：<img src="/2022/07/03/diffusion_map/prl2.png" class="" title="This is an image"> - 选择不同的<span class="math inline">\(\epsilon\)</span>可以得到不同的相图 - 选择合适的<span class="math inline">\(\epsilon\)</span>，可以很好根据聚类的数目，区分出来三种相，和物理的结果吻合。</p><h2 id="dmm数学简化">DMM数学简化</h2><p>对于给定的一个核矩阵<span class="math inline">\(K\)</span>，我们可以得到转移矩阵<span class="math inline">\(P=D^{-1}K\)</span>，正如之前说的。显然，<span class="math inline">\(P\)</span>在大部分的情况下，都不是一个实对称的矩阵，我们可以通过简单的转化，分析<span class="math inline">\(Q =D^{-\frac{1}{2}}KD^{-\frac{1}{2}}\)</span>来探索<span class="math inline">\(P\)</span>的一些性质。很显然，</p><ul><li><span class="math inline">\(P\)</span>和<span class="math inline">\(Q\)</span>是一对相似矩阵</li><li><span class="math inline">\(Q\)</span>是实对称的</li></ul><p>那么我们可以轻松地在实数域内，对<span class="math inline">\(Q\)</span>做对角化，得到其对应的特征值<span class="math inline">\(1=\lambda_0\ge \lambda_i \ge \cdots \ge\lambda_{N-1} \ge 0\)</span> 和对应的特征向量<span class="math inline">\(v_0, v_1, \cdots,v_{N-1}\)</span>。那么我们很容易发现<span class="math inline">\(P\)</span>矩阵的右特征向量为<span class="math inline">\(u_0:=D^{-\frac{1}{2}}v_0,u_1:=D^{-\frac{1}{2}}v_1, \cdots\)</span> 和 左特征向量 <span class="math inline">\(w_0 := D^{\frac{1}{2}}v_0, w_1 =D^{\frac{1}{2}}v_1, \cdots\)</span>。那么<span class="math inline">\(P\)</span>的谱分解可以写成： <span class="math display">\[P = \sum_{j=0}^{N-1}\lambda_j u_j w_j^T\]</span> 同时<span class="math inline">\(t\)</span>-step的转移矩阵可以写成: <span class="math display">\[P^t = \sum_{j=0}^{N-1}\lambda_j^t u_j w_j^T\]</span> 那么在<span class="math inline">\(\{w_j\}_{j=1}^{N-1}\)</span>构成的坐标系下面，第<span class="math inline">\(i\)</span>组数据的表达就是<span class="math inline">\((\lambda_0^t u_0(i), \lambda_1^t u_1(i), \cdots,\lambda_{N-1}^t u_{N-1}(i))\)</span>。使用者可以根据需求对<span class="math inline">\(0\)</span>到<span class="math inline">\(N-1\)</span>之间的dimension选择适合的维度进行，然后做数据降维。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 量子物理，机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>因果分析</title>
      <link href="/2022/05/01/nonlinear_analysis/"/>
      <url>/2022/05/01/nonlinear_analysis/</url>
      
        <content type="html"><![CDATA[<h1 id="非线性因果分析技术">非线性因果分析技术</h1><p>在基础的统计学中，我们学过用协方差矩阵来表征两个高维随机变量<span class="math inline">\(X=(X_1,X_2,\cdots,X_n)^T, Y=(Y_1,Y_2,\cdots,Y_m)^T\)</span>之间的相关性。但是，我们知道相关性并不意味着独立性，更加不能准确描述因果性。这篇Blog，我们将持续更新一些因果性分析工具。<a id="more"></a></p><h2 id="granger-causalityg-causality">GrangerCausality(G-causality)</h2><p>所谓的G-causality的想法其实非常的简单的，就是物理学中常说的控制变量法。</p><blockquote><p>Definition 假设我们拥有三个随机过程<span class="math inline">\(X_t,Y_t, Z_t\)</span>, 并且我们将其各自的realization trajectory标记为<span class="math inline">\(X^t, Y^t, Z^t\)</span>, 对于给定的一个度量<span class="math inline">\(g\)</span>, 和拟合算子<span class="math inline">\(f_1, f_2\)</span>, 我们很容易定义对应的误差: <span class="math display">\[\mathcal{R}(Y^{t+1}| Y^t, Z^t) = g(Y_{t+1}, f_1(Y^t, Z^t)) \\\mathcal{R}(Y^{t+1}| Y^t, X^t, Z^t) = g(Y_{t+1}, f_1(Y^t, X^t, Z^t))\]</span></p></blockquote><p>然后我们检验方法，检验两个残差有没有显著的差异，来表明<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>有没有显著的因果关系。</p><p>当然我们需要注意的是:</p><ul><li>整个流程不是完全准确的，因为选择的拟合算子<span class="math inline">\(f_1,f_2\)</span>是非常关键。在最理想的情况下，其实我们还需要对整个<span class="math inline">\(f\)</span>-space 去做一轮优化才是。</li><li>如果<span class="math inline">\(f_1,f_2\)</span>选成线性算子，整个流程和自回归非常相近。该检验可以和很多自回国技术结合起来。</li></ul><h2 id="transer-entropy">Transer Entropy</h2><p>除了拟合数据的角度，我们还可以从更加理论的角度去分析随机变量的因果性，即从两者对应的分布情况,而不再依赖额外从分布到实数的映射构造的随机变量。在信息论领域，entropy作为一个非常重要的统计量来衡量分布的性质。那么我们很自然地，我们就可以想到条件熵来描述因果性的部分。</p><blockquote><p>条件熵 给定两个随机变量<span class="math inline">\(X, Y\)</span>,<span class="math inline">\(Y\)</span>之于<span class="math inline">\(X\)</span>的条件熵可以定义为 <span class="math display">\[H(Y|X) = H(X,Y) - H(X)\]</span></p><p>对于之前没有接触过信息论的读者，这边简单地解释一下熵和条件熵的物理意义。所谓熵即代表该分布对应的不确定性。当人们讨论随机的时候，常常直觉会去感觉到有的随机事件没有那么"随机"，大部分时候你能猜到，但是有的随机事件，感觉就很真的很“随机”，完全没有头绪。熵则是量化直觉上有多随机的数学量。那么条件熵又是怎么回事呢？其实就是在你们没啥信息的感觉<span class="math inline">\(Y\)</span>这个变量大概有<span class="math inline">\(H(Y)\)</span>这么多的不确定性。但是如果你知道了<span class="math inline">\(X\)</span>的信息之后，有可能可以帮助你了解到<span class="math inline">\(Y\)</span>，那么你对<span class="math inline">\(Y\)</span>的不确定性就会有可能降低，降低到多少呢，就是<span class="math inline">\(H(Y|X)\)</span>.</p></blockquote><p>当然如果<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>是独立的，从上面的解释我们就会期望<span class="math inline">\(H(Y|X)=H(Y)\)</span>。这点也是很好验证的。</p><blockquote><p>转移熵 给定三个随机过程<span class="math inline">\(X_t,Y_t,Z_t\)</span>, 我们定义<span class="math inline">\(X\)</span>到<span class="math inline">\(Y\)</span>的转移熵为 <span class="math display">\[T(X \to Y) = H(Y_{t+1}|Y^t, Z^t) - H(Y_{t+1}|Y^t, X^t, Z^t)\]</span></p></blockquote><p>当然有人为了保证对称性，也会采用下面的定义来表示信息的净流出， &gt;信息净流出 &gt; <span class="math display">\[&gt; \tilde{T}(X \to Y) = T(X \to Y) - T(Y \to X)&gt; \]</span></p><h2 id="附录">附录</h2><h3 id="g-causality和转移熵的等价性">G-causality和转移熵的等价性</h3><p>有一个非常有意思的结果，就是在假设<span class="math inline">\(X_t,Y_t\)</span>均服从多元高斯分布的条件下面。在2009年的一篇<a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.103.238701"><strong>PRL</strong></a>证明了G-causality和转移熵的等价性。这就是建立数学拟合和信息论之间的一些桥梁，也建立了实操(G-causality更加好实现)和理论(转移熵实现更为麻烦，要从数据去得到分布不是那么得容易)的关系。</p><p>这边简要介绍一些证明的核心内容。 #### G-causality</p><p>给定两个随机变量<span class="math inline">\(X,Y\)</span>, 分别为<span class="math inline">\(n\)</span>元，<span class="math inline">\(m\)</span>元，线性回归的表达则是， <span class="math display">\[Y = A X + b + \epsilon\]</span> 这边<span class="math inline">\(A\in R^{m\times n}, b \inR^{m}\)</span> 为线形拟合的参数，<span class="math inline">\(\epsilon\)</span>为残差项。如果<span class="math inline">\(X,Y\)</span>均服从高斯分布的，那么使用<span class="math inline">\(tr(\Sigma(\epsilon))\)</span>作为损失函数去优化，则是符合MLE的一种算法。也就是我们要优化的是， <span class="math display">\[\mathcal{L} = \mathbb{E}[(Y - AX - b)^T (Y -AX -b)]\]</span> 对<span class="math inline">\(\mathcal{L}\)</span>求导，我们可以得到 <span class="math display">\[\frac{\partial \mathcal{L}}{\partial A} = -2\mathbb{E}[(Y-AX-b)X^T] \\\frac{\partial \mathcal{L}}{\partial b} = -2\mathbb{E}[Y-AX-b]\]</span> 令导数等于<span class="math inline">\(0\)</span>，我们可以得到<span class="math inline">\(A,b\)</span>解分别为 <span class="math display">\[A = \Sigma(Y,X)\Sigma(X,X)^{-1} \\b = \mathbb{E}[Y] - A\mathbb{E}[X]\]</span> 那么我们容易得到， <span class="math display">\[\Sigma(\epsilon,\epsilon) = \mathbb{E}[(Y-AX-b)(Y-Ax-b)^T] \\= \Sigma(Y,Y) - A \Sigma(X,Y) \\= \Sigma(Y,Y) - \Sigma(Y,X)\Sigma(X,X)^{-1}\Sigma(X,Y)\]</span> 为了方便，我们将最后一项简记成<span class="math inline">\(\Sigma(Y|X)\)</span>。</p><p>那么我们回到G-causality的讨论上面来，同时这边将<span class="math inline">\(f_1,f_2\)</span>限制成线性的拟合算子。正如前面所说的，G-causality就是要比较加入<span class="math inline">\(X^t\)</span>之后，对于<span class="math inline">\(Y^{t+1}\)</span>的拟合带来了多少的增益。那么这边文章作者采用了对数ratio的方法来比较，<span class="math display">\[\mathcal{G}(X \to Y) := \ln \frac{\det(\Sigma(Y^{t+1}|Y^t \oplusZ^t))}{\det(\Sigma(Y^{t+1}|Y^t \oplus X^t \oplus Z^t))}\]</span></p><h4 id="转移熵">转移熵</h4><p>这部分我们要证明的是<span class="math inline">\(X\)</span>到<span class="math inline">\(Y\)</span>的转移熵<span class="math inline">\(T(X\to Y)\)</span>等价于上面的<span class="math inline">\(\mathcal{G}(X \to Y)\)</span></p><p>给定一个服从<span class="math inline">\(n,m\)</span>元高斯分布的随机变量<span class="math inline">\(X, Y\)</span>,我们很容易证明其对应的熵的表达式如下： <span class="math display">\[H(X) = \frac{1}{2}\ln \det(\Sigma(X)) + \frac{n}{2}\ln(2\pi e),\\H(X|Y) = H(X\oplus Y) - H(Y) = \frac{1}{2} \ln \frac{\det (\Sigma(X\oplus Y))}{\det \Sigma(Y)} + \frac{n}{2}\ln(2 \pi e)\]</span> 同时， <span class="math display">\[\Sigma(X\oplus Y) = \left(\begin{matrix}\Sigma(X,X) &amp; \Sigma(X,Y) \\ \Sigma(Y,X) &amp; \Sigma(Y,Y)\\\end{matrix}\right)\]</span> 对于<span class="math inline">\(2\times2\)</span>的分块矩阵，我们很容易其特征值，得到 <span class="math display">\[\det\Sigma(X \oplus Y) = \det\Sigma(Y,Y)\det\Sigma(X|Y)\]</span></p><p>那么对于转移熵, <span class="math display">\[T(X \to Y) = H(Y^{t+1}|Y^t, Z^t) - H(Y^{t+1}|Y^t, X^t, Z^t) \\= \frac{1}{2} \mathcal{G}(X \to Y)\]</span></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《大衰退读书笔记》</title>
      <link href="/2022/03/20/%E5%A4%A7%E8%A1%B0%E9%80%80/"/>
      <url>/2022/03/20/%E5%A4%A7%E8%A1%B0%E9%80%80/</url>
      
        <content type="html"><![CDATA[<h1 id="大衰退">大衰退</h1><p>这本书主要从资产负债表衰退的角度来分析日本消失的20年。同时作者也利用资产负债表衰退的概念来重新诠释<strong>流动性陷阱</strong><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <a id="more"></a></p><h2 id="个人感悟">个人感悟</h2><h3 id="什么是资产负债表衰退">什么是资产负债表衰退</h3>资产负债表衰退，通常是指，由于各类资产价格<strong>暴跌</strong>，导致私营企业从扩大经营规模，扩大利润来源等等原始的的目的，转化以降低企业负债为目的，从而导致企业的资金需求不足，进而导致通货紧缩形式的经济衰退。<pre class="mermaid">graph TD;G1[资产价格暴跌] --社会文化还是什么深层原因 --&gt; G2[企业降低负债];G2 --资金流向银行 --&gt; G3[通货紧缩];G3 --&gt; G4[经济衰退];</pre><p>这整个流程还是有一些值得商榷的地方。</p><ul><li>资产价格暴跌 --&gt; 资产负债表的难看 --&gt;企业开始偿债。作者认为这个过程中，货币政策已经失效，但是作为一个企业经营者，面对低成本（低利率）的资金，如果企业本身盈利模式没有出现问题，那完全可以通过：借贷 --&gt; 扩大生产 --&gt; 更多的利润 --&gt; 购置更多的资产，这套流程来解决资产负债表的问题。<strong>到底是什么因素导致了大部分的日本企业选择了，以降低负债来解决资产负债表的问题</strong></li></ul><h3 id="资产负债表衰退是日本经济衰退的佐证">资产负债表衰退是日本经济衰退的佐证</h3><ul><li>不是结构性的问题：因为日本没有结构性问题带来的种种弊端：例如，工人罢工，巨额贸易逆差，货币贬值。<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></li><li>数据佐证了，企业对资金的需求确实在降低，而且并非由于银行采取了信贷紧缩政策。</li></ul><h3 id="资产负债表衰退背景下各个组件的关系">资产负债表衰退背景下，各个组件的关系</h3><pre class="mermaid">graph LRA1[企业]--需求降低--&gt;B1A1 -- 偿债 --&gt; B1A1 -- 偿债 --&gt; C4subgraph Bank[银行]    B1[资金闲置]endsubgraph Gov[财政]    C1[财政支出增加]    C2[财政收入减少]    C4[降低税收]    C4 --&gt; C2 --&gt; C8[财政赤字]    C6[国债]    C6 --&gt; C1 --&gt; C8endsubgraph Center[货币]    D1[利率降低]endB1 -- 促进资金需求 --&gt; D1B1 -- 缓解通货紧缩 --&gt; C6</pre><ul><li>如果资产负债大幅度衰退，货币政策失效，只剩下财政这个手段来调控，政府此刻应该杜绝财政整顿。</li><li>汇率政策，降低汇率，对于贸易逆差的国家可以在缓解资产负债衰退。<ul><li>在一个经济正常的世界里，市场可以如此调节汇率的平衡。<code>降低汇率</code>--&gt;<code>出口增加</code>--&gt;<code>刺激国内经济</code>--&gt;<code>进口增加</code>--&gt;<code>贸易平衡</code>。</li><li>在资产负债表衰退的世界里，汇率降低的影响如下，<code>降低汇率</code>--&gt;<code>出口增加</code>--&gt;<code>刺激国内经济</code>--&gt;<code>企业赚来的利润去还债</code></li><li>因此在资产负债表衰退的世界里，贸易顺差的国家，降低会使得贸易顺差进一步扩大，这会导致其他国家，采取对应的汇率政策，以阻止该国的汇率的降低。但是对于贸易逆差的国家，只会降低贸易逆差，不会导致外界对其有很明显的反向操作。</li></ul></li></ul><h3 id="阴阳周期理论">阴阳周期理论</h3><pre class="mermaid">graph LRsubgraph Neg[阴周期]    N1[收紧货币泡沫破灭]    N1 --&gt; N2[资产负债表衰退]    N2 --&gt; N3[负债最小化+财政政策]    N3 --&gt; N4[完成偿债-利依然低迷]endsubgraph Pos[阳周期]    P1[资金需求复兴]    P1 --&gt; P2[转为货币政策]    P2 --&gt; P3[私营企业充满活力]    P3 --&gt; P4[私营企业过渡自信]endM1[泡沫] --&gt; N1M2[债务抵触综合征] --&gt; P1N4 --&gt; M2P4 --&gt; M1</pre><p>在阴阳周期理论中需要注意的：</p><ul><li>阴周期，并不一定意味总的经济下行，但是私营企业的经济活动肯定比较萎靡。</li><li><code>大周期</code>和<code>小周期</code>的轮转。</li><li>阴阳周期最主要的区别在于<code>私营企业的的财务状况</code></li></ul><p><strong>所谓的宏观经济学新的圣经</strong> 阳周期：</p><ul><li>新古典主义</li><li>货币主义</li><li>凯恩斯主义</li><li>新凯恩斯主义</li></ul><p>阴周期：</p><ul><li>资产负债表衰退理论</li></ul><table><thead><tr class="header"><th></th><th>阳的世界</th><th>阴的世界</th></tr></thead><tbody><tr class="odd"><td>现象</td><td>教科书的经济学</td><td>资产负债表衰退</td></tr><tr class="even"><td>法则</td><td>亚当斯密的“看不见的手”</td><td>合成谬误<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></td></tr><tr class="odd"><td>企业财务</td><td>资产&gt;负债</td><td>资产&lt;负债</td></tr><tr class="even"><td>行动原理</td><td>利润最大化</td><td>负债最小化</td></tr><tr class="odd"><td>结果</td><td>最大多数的幸福</td><td>置之不理则将导致萧条</td></tr><tr class="even"><td>货币政策</td><td>有效</td><td>无效(流动性陷阱)</td></tr><tr class="odd"><td>财政政策</td><td>负效应(挤出效应)</td><td>有效</td></tr><tr class="even"><td>物价</td><td>通货膨胀</td><td>通货紧缩</td></tr><tr class="odd"><td>利率</td><td>正常</td><td>极低</td></tr><tr class="even"><td>储蓄</td><td>美德</td><td>恶徳</td></tr><tr class="odd"><td>银行危机对策</td><td>优惠和迅速处理不良贷款</td><td>资本注入和谨慎处理不良贷款</td></tr></tbody></table><h3 id="几个有意思的观点">几个有意思的观点</h3><ul><li>定量宽松政策的解除不等于金融紧缩政策<ul><li>央行向商业银行出售国债，清除流动性，在一般的情况下，因为商业银行，没有用来购买国债的闲置资金，所以抛售金融资产，导致金融资产价格降低，经济热度下降，金融趋于紧缩。但是在资产负债表期间，商业银行有足够的闲置资金来，购买国债，不够成金融紧缩。</li></ul></li><li>财政赤字和GDP的比率没有增长并不能说明财政政策的失效。因为有两种情况，第一种，就是财政支出和税收确实没有大幅度变化，这种情况下，财政政策影响甚微；第二种，财政支出增加，刺激了经济增长，因而税收也同步增加，所以赤字比率没有改变。在这种情况下，扩张的的财政政策明显起到了很大的作用。</li><li></li></ul><pre class="mermaid">graph LRA1[经济衰退] --&gt; A2[经济周期造成的衰退] --&gt; B1[货币政策]A1 --&gt; A3[资产负债表衰退] --&gt; B2[财政政策]</pre><ul><li>泡沫之所以成为泡沫，就是因为资产价格水平再也无法通过现金流量折现法来判断。</li></ul><h2 id="摘要">摘要</h2><ul><li>福井俊彦：只有等到私营部门的资金需求变得更加强劲之后，政府的财政改革与之配套，这样才不会出现问题。</li><li>传统经济学的大前提假设：假设那些导致经济脱离正常轨道的外部冲击的性质并不十分重要，因此，即使收到外部冲击，受其影响的经济行为者的既定目标也不会改变。资产负债表衰退这一概念主张，特定的外部冲击会从根本上改变企业和个人的行为目标。</li><li>中央银行绝对不可以承担过多的风险，这是因为今天的货币完全是由中央银行的信用来担保，而非黄金和白银<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>。</li><li>只要一个国家的中央银行能够正常运作，那么无论再大的政府预算赤字，其导致的高利率和私人投资挤出效应的负面影响都是有限的，但是一旦中央银行迫于政府压力，执行有损公共信赖的政策，则必将承受灾难性的后果。</li><li>凯恩斯主义的重大缺陷：<ul><li>完全没有考虑资产负债表的问题</li><li>将利率过低导致金融政策失灵绝对地归咎于，流动性偏好使得公众将资金从债券转为现金，而不考虑流动性偏好中，借贷方的需求转换</li><li>没有解释对经济和资产价格影响显著的货币政策为什么失灵 ## 数据</li></ul></li><li>查看全体银行的资产负债表，可以得到各组件的借贷信息，以及全国范围的货币供应量，货币乘数等等指标。</li><li>存款周转率在某种程度上可以衡量资产价格的泡沫大小。逻辑来自于，金融资产过热的时候，会引发民众的大量交易。</li></ul><h2 id="建模">建模</h2><ul><li>债务和资金需求的矛盾<ul><li>寻找一个模型来描述，资产下跌的程度与还债的关系，以此寻求量化资产负债表衰退的程度。</li></ul></li></ul><h2 id="脚注">脚注</h2><aside id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr><ol><li id="fn1"><p>利率低至债券收益等同于货币，资金供应大量囤积货币的行为。<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn2"><p>贸易逆差--&gt;货币贬值；贸易顺差--&gt;货币升值的原理：当一个国家有巨大贸易逆差的时候，即进口量&gt;出口量。进口企业需要将本国货币兑成外币用来购买，出口企业会将赚来的外币换成本国货币，进口量大的时候，对外币的需求就会大，根据供给需要关系，对应的外币升值，本国货币贬值。<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn3"><p>每个个体都做局部正确的事情，但是全局的效益却因此降低。<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn4"><p>如果中国央行过分超发货币，人们对央行的信任是不是并不会像西方那样迅速降低。在这种情况下，央行的超发货币，并不会在短时间内提升人们对于通货膨胀率上升的预期。<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></aside><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 投资 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 宏观投资 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Start with Hexo</title>
      <link href="/2022/03/19/start-with-hexo/"/>
      <url>/2022/03/19/start-with-hexo/</url>
      
        <content type="html"><![CDATA[<h1 id="依赖环境准备"><a href="#依赖环境准备" class="headerlink" title="依赖环境准备"></a>依赖环境准备</h1><h2 id="Node-js安装"><a href="#Node-js安装" class="headerlink" title="Node.js安装"></a><code>Node.js</code>安装</h2><p><code>Hexo</code>是基于<code>Node.js</code>安装的，因此需要先安装<code>Node.js</code>, 安装地址为 <a href="https://nodejs.org/en/download/" target="_blank" rel="noopener">Node.js</a><br><a id="more"></a></p><p>安装Node.js会包含环境变量及npm的安装, 安装完成后可以检查<code>Node.js</code>是否安装成功<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node -v</span><br></pre></td></tr></tbody></table></figure><br>检查npm是否安装成功<br><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm -v</span><br></pre></td></tr></tbody></table></figure><br>检测成功后就安装完成了<p></p><h1 id="Hexo安装及初始化"><a href="#Hexo安装及初始化" class="headerlink" title="Hexo安装及初始化"></a><code>Hexo</code>安装及初始化</h1><p><a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">Hexo</a>是搭建个人网站的框架，安装方式如下<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 安装hexo</span></span><br><span class="line">sudo npm install hexo-cli -g</span><br><span class="line"></span><br><span class="line"><span class="comment">## cd到希望建立博客的文件夹目录下, 初始化blog文件夹</span></span><br><span class="line">hexo init blog</span><br><span class="line"><span class="built_in">cd</span> blog</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="Hexo本地使用"><a href="#Hexo本地使用" class="headerlink" title="Hexo本地使用"></a><code>Hexo</code>本地使用</h1><h2 id="Hexo基本使用命令"><a href="#Hexo基本使用命令" class="headerlink" title="Hexo基本使用命令"></a><code>Hexo</code>基本使用命令</h2><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 新建一篇博客</span></span><br><span class="line">hexo n my_first_site</span><br><span class="line"><span class="comment"># hexo new post --path filepath name  # 指定生成md的地址</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 生成静态文件</span></span><br><span class="line">hexo g</span><br><span class="line"></span><br><span class="line"><span class="comment">## 本地运行server</span></span><br><span class="line">hexo s</span><br><span class="line"></span><br><span class="line"><span class="comment">## 清除缓存</span></span><br><span class="line">hexo clean</span><br></pre></td></tr></tbody></table></figure><ul><li><p>运行完<code>hexo n my_first_site</code>后，可以在 <code>blog/source/_posts</code>文件夹下发现多了一个<code>my_first_site.md</code>文件，此文件即我们编辑网页内容的主要源文件</p></li><li><p>运行完<code>hexo s</code>之后，terminal会返回我们本地访问的网址，可以在上面查看预览样式</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">INFO  Validating config</span><br><span class="line">INFO  Start processing</span><br><span class="line">INFO  Hexo is running at http://localhost:4000/ . Press Ctrl+C to stop.</span><br></pre></td></tr></tbody></table></figure></li></ul><h2 id="Hexo主题的选择"><a href="#Hexo主题的选择" class="headerlink" title="Hexo主题的选择"></a><code>Hexo</code>主题的选择</h2><h3 id="Next主题"><a href="#Next主题" class="headerlink" title="Next主题"></a><code>Next</code>主题</h3><ol><li>下载next主题<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 下载next主题到blog/themes/下</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/theme-next/hexo-theme-next themes/next</span><br></pre></td></tr></tbody></table></figure></li><li>修改Hexo配置文件使主题生效，将<code>blog/_config.yml</code>文件中的<code>theme</code>设定成<code>next</code><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Extensions</span></span><br><span class="line"><span class="comment">## Plugins: https://hexo.io/plugins/</span></span><br><span class="line"><span class="comment">## Themes: https://hexo.io/themes/</span></span><br><span class="line">theme: next</span><br></pre></td></tr></tbody></table></figure></li></ol><p>完成之后重新运行<code>Hexo g</code>以及<code>Hexo s</code>即可预览验证主题是否生效</p><h1 id="Hexo-的远程部署"><a href="#Hexo-的远程部署" class="headerlink" title="Hexo 的远程部署"></a><code>Hexo</code> 的远程部署</h1><h2 id="github-创建自己用户名的个人仓库"><a href="#github-创建自己用户名的个人仓库" class="headerlink" title="github 创建自己用户名的个人仓库"></a>github 创建自己用户名的个人仓库</h2><p>ref: <a href="https://snailwish.com/368/" target="_blank" rel="noopener">从零开始，一步一步教你用 Github 快速搭建免费的个人主页</a></p><h2 id="Hexo-和远程git仓库绑定"><a href="#Hexo-和远程git仓库绑定" class="headerlink" title="Hexo 和远程git仓库绑定"></a><code>Hexo</code> 和远程git仓库绑定</h2><p>将<code>blog/_config.yml</code>文件中的<code>deploy</code>设定成指定的远程部署网页即可, 如下所示<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/one-command-deployment</span></span><br><span class="line">deploy:</span><br><span class="line">  <span class="built_in">type</span>: git</span><br><span class="line">  repo: https://github.com/{username}/{username}.github.io</span><br><span class="line">  branch: master</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="Hexo-在远程网页上部署"><a href="#Hexo-在远程网页上部署" class="headerlink" title="Hexo 在远程网页上部署"></a><code>Hexo</code> 在远程网页上部署</h2><p>在生成好静态网页之后，直接运行命令<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部署网页</span></span><br><span class="line">Hexo d</span><br></pre></td></tr></tbody></table></figure><p></p><p>这样最简单的网页部署就完成了，后面只需要修改<code>blog/source/_posts/*.md</code>文件即可。</p><h1 id="Hexo高级配置"><a href="#Hexo高级配置" class="headerlink" title="Hexo高级配置"></a><code>Hexo</code>高级配置</h1><h2 id="一般主题配置"><a href="#一般主题配置" class="headerlink" title="一般主题配置"></a>一般主题配置</h2><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 修改作者, 文件名: (/_config.yml)</span></span><br><span class="line">author: fishermanxx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 修改标题下方菜单栏, 添加tag以及category栏, 文件名: (/themes/next/_config.yml)</span></span><br><span class="line"><span class="comment">##（另外需要新建一个tags页面去展示, 此时未生效, detail见 文章分类/分类展示 ）</span></span><br><span class="line">menu:</span><br><span class="line">  home: / || fa fa-home</span><br><span class="line">  tags: /tags/ || fa fa-tags</span><br><span class="line">  categories: /categories/ || fa fa-th</span><br><span class="line">  archives: /archives/ || fa fa-archive</span><br><span class="line"></span><br><span class="line"><span class="comment">## 添加浏览次数等统计量</span></span><br><span class="line"><span class="comment">### Step1. 修改/themes/next/_config.yml文件中的busuanzi_count:</span></span><br><span class="line">busuanzi_count:</span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">  total_visitors: <span class="literal">false</span></span><br><span class="line">  total_visitors_icon: fa fa-user</span><br><span class="line">  total_views: <span class="literal">false</span></span><br><span class="line">  total_views_icon: fa fa-eye</span><br><span class="line">  post_views: <span class="literal">true</span>  <span class="comment">## 文章访问量</span></span><br><span class="line">  post_views_icon: fa fa-eye</span><br><span class="line"><span class="comment">### Step2. 统计字数等， 安装插件， bash运行</span></span><br><span class="line">npm install hexo-symbols-count-time --save </span><br><span class="line"><span class="comment">### Step3. 在/_config.yml中配置</span></span><br><span class="line">symbol_count_time:</span><br><span class="line">  symbols: <span class="literal">true</span>  <span class="comment"># 是否统计字数</span></span><br><span class="line">  time: <span class="literal">false</span>  <span class="comment"># 是否统计阅读时长</span></span><br><span class="line">  total_symbols: <span class="literal">false</span>  <span class="comment">#总字数</span></span><br><span class="line">  total_time: <span class="literal">false</span>  <span class="comment">#总时长</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 添加文章结束标志</span></span><br><span class="line"><span class="comment">### Step1. 在/themes/next/layout/_macro 新建 passage-end_tag.swig, 并添加一下内容</span></span><br><span class="line">&lt;div&gt;</span><br><span class="line">    {% <span class="keyword">if</span> not is_index %}</span><br><span class="line">        &lt;div style=<span class="string">"text-align:center;color: #ccc;font-size:14px;"</span>&gt;-------------------The END-------------------&lt;/div&gt;</span><br><span class="line">    {% endif %}</span><br><span class="line">&lt;/div&gt;</span><br><span class="line"><span class="comment">### Step2. 在/themes/next/layout/_macro/post.swig文件中，在 END POST BODY之后输入</span></span><br><span class="line">&lt;div&gt;</span><br><span class="line">  {% <span class="keyword">if</span> not is_index %}</span><br><span class="line">      {% include <span class="string">'passage-end-tag.swig'</span> %}</span><br><span class="line">  {% endif %}</span><br><span class="line">&lt;/div&gt;</span><br><span class="line"><span class="comment">### Step3. 在/themes/next/_config.yml 末尾添加</span></span><br><span class="line">passage_end_tag:</span><br><span class="line">  enabled: <span class="literal">true</span></span><br></pre></td></tr></tbody></table></figure><h2 id="自定义hexo-new生成模板"><a href="#自定义hexo-new生成模板" class="headerlink" title="自定义hexo new生成模板"></a>自定义hexo new生成模板</h2><p>hexo new (post/page/draft) $filename的模板，修改 <code>/scaffolds/post.md</code>文件(<code>page.md</code>和<code>draft.md</code>文件同理)<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">title: {{ title }}</span><br><span class="line">date: {{ date }}</span><br><span class="line">tags:                <span class="comment">#标签</span></span><br><span class="line">categories:      <span class="comment">#分类</span></span><br><span class="line">copyright: <span class="literal">true</span> <span class="comment">#版权声明</span></span><br><span class="line">permalink: 01  <span class="comment">#文章链接，有默认值</span></span><br><span class="line">top: 0              <span class="comment">#置顶优先级</span></span><br><span class="line">password:      <span class="comment">#密码保护</span></span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="文章分类"><a href="#文章分类" class="headerlink" title="文章分类"></a>文章分类</h2><h3 id="分类展示"><a href="#分类展示" class="headerlink" title="分类展示"></a>分类展示</h3><p>此处仅展示tags的设置, categories同理设定即可</p><ol><li><p>建立分类展示的静态页面</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page tags</span><br></pre></td></tr></tbody></table></figure><p>发现生成一个<code>/source/tags/index.md</code>文件</p></li><li><p>设置页面类型, 直接在<code>index.md</code>文件中输入:</p><figure class="highlight markdown"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: Tags</span><br><span class="line">date: 2022-03-21 00:50:56</span><br><span class="line">type: "tags"</span><br><span class="line">comments: false</span><br><span class="line">---</span><br></pre></td></tr></tbody></table></figure></li><li><p>修改菜单栏, 修改<code>themes/${theme_name}/_config.yml</code>文件，修改下列设置</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: /</span><br><span class="line">  archives: /archives</span><br><span class="line">  tags: /tags</span><br></pre></td></tr></tbody></table></figure></li><li><p>修改tags页面的字体大小, 修改文件<code>themes/${theme_name}/_config.yml</code>, 如下</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TagCloud settings for tags page.</span></span><br><span class="line">tagcloud:</span><br><span class="line">  <span class="comment"># All values below are same as default, change them by yourself.</span></span><br><span class="line">  min: 30 <span class="comment"># Minimun font size in px</span></span><br><span class="line">  max: 50 <span class="comment"># Maxium font size in px</span></span><br><span class="line">  start: <span class="string">"#ccc"</span> <span class="comment"># Start color (hex, rgba, hsla or color keywords)</span></span><br><span class="line">  end: <span class="string">"#111"</span> <span class="comment"># End color (hex, rgba, hsla or color keywords)</span></span><br><span class="line">  amount: 200 <span class="comment"># Amount of tags, change it if you have more than 200 tags</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>修改文章末尾的tag的图标, 修改文件<code>/themes/next/layout/_macro/post.swig</code>,替换</p><figure class="highlight"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;a href="{{ url_for(tag.path) }}" rel="tag"&gt;{{ tag_indicate }} {{ tag.name }}&lt;/a&gt;</span><br></pre></td></tr></tbody></table></figure><p>为</p><figure class="highlight"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;a href="{{ url_for(tag.path) }}" rel="tag"&gt;&lt;i class="fa fa-tag"&gt;&lt;/i&gt; {{ tag.name }}&lt;/a&gt;</span><br></pre></td></tr></tbody></table></figure></li></ol><h3 id="分类存储"><a href="#分类存储" class="headerlink" title="分类存储"></a>分类存储</h3><p>通过修改配置文件<code>blog/_config.yml</code>中的<code>new_post_name</code>来让创建的文件按照时间分类，也可以按照(<code>hexo new post --path filepath name</code>)来创建，示例如下<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">permalink: :year/:month/:day/:title/</span><br><span class="line">new_post_name: :year/:month/:title.md <span class="comment"># File name of new posts</span></span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="文章加密访问"><a href="#文章加密访问" class="headerlink" title="文章加密访问"></a>文章加密访问</h2><h3 id="简单方法"><a href="#简单方法" class="headerlink" title="简单方法"></a>简单方法</h3><ol><li>打开主题目录下layout/_partials/head.swig文件,在meta标签后面插入这样一段代码<figure class="highlight javascript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;script&gt;</span><br><span class="line">    (<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>{</span><br><span class="line">        <span class="keyword">if</span>(<span class="string">'{{ page.password }}'</span>){</span><br><span class="line">            <span class="keyword">if</span> (prompt(<span class="string">'请输入文章密码'</span>) !== <span class="string">'{{ page.password }}'</span>){</span><br><span class="line">                alert(<span class="string">'密码错误！'</span>);</span><br><span class="line">                history.back();</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    })();</span><br><span class="line">&lt;<span class="regexp">/script&gt;</span></span><br></pre></td></tr></tbody></table></figure></li><li>在每篇文章md的开头添加 <code>password: pwd</code>, 即可以激活<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: Start with Hexo</span><br><span class="line">date: 2022-03-19 00:06:40</span><br><span class="line">tags: hexo</span><br><span class="line"><span class="comment"># password: mypwd</span></span><br><span class="line">---</span><br></pre></td></tr></tbody></table></figure><h3 id="安装插件方法"><a href="#安装插件方法" class="headerlink" title="安装插件方法"></a>安装插件方法</h3>ref: <a href="https://www.itfanr.cc/2021/04/16/hexo-blog-article-encryption/" target="_blank" rel="noopener">hexo博客文章加密</a></li><li><p>安装插件</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-blog-encrypt</span><br></pre></td></tr></tbody></table></figure></li><li><p>在/_config.yml中输入响应配置</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Security</span></span><br><span class="line">encrypt: <span class="comment"># hexo-blog-encrypt</span></span><br><span class="line">  silent: <span class="literal">true</span></span><br><span class="line">  abstract: 这是一篇加密文章，需要密码才能继续阅读。</span><br><span class="line">  message: 当前文章暂不对外可见，请输入密码后查看！</span><br><span class="line">  tags:</span><br><span class="line">  - {name: private, password: mypwd}</span><br><span class="line">  wrong_pass_message: 抱歉，您输入的密码错误，请检查后重新输入。</span><br><span class="line">  wrong_hash_message: 抱歉, 当前文章不能被校验, 不过您还是可以看看解密后的内容</span><br></pre></td></tr></tbody></table></figure></li><li><p>解决解密后目录消失问题，找到文件 <code>themes/next/layout/_macro/sidebar.swig</code> ，编辑如下部分：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;aside class=<span class="string">"sidebar"</span>&gt;</span><br><span class="line">  &lt;div class=<span class="string">"sidebar-inner"</span>&gt;</span><br><span class="line"></span><br><span class="line">    {%- <span class="built_in">set</span> display_toc = page.toc.enable and display_toc %}</span><br><span class="line">    {%- <span class="keyword">if</span> display_toc %}</span><br><span class="line">      </span><br><span class="line">      <span class="comment">## Start</span></span><br><span class="line">      {%- <span class="keyword">if</span> (page.encrypt) %}</span><br><span class="line">        {%- <span class="built_in">set</span> toc = toc(page.origin, { class: <span class="string">"nav"</span>, list_number: page.toc.number, max_depth: page.toc.max_depth }) %}</span><br><span class="line">      {%- <span class="keyword">else</span> %}</span><br><span class="line">        {%- <span class="built_in">set</span> toc = toc(page.content, { class: <span class="string">"nav"</span>, list_number: page.toc.number, max_depth: page.toc.max_depth }) %}</span><br><span class="line">      {%- endif %}</span><br><span class="line">      <span class="comment">## End</span></span><br><span class="line">      <span class="comment">## 同时删除 set toc这一句！！！</span></span><br><span class="line"></span><br><span class="line">      {%- <span class="built_in">set</span> display_toc = toc.length &gt; 1 and display_toc %}</span><br><span class="line">    {%- endif %}</span><br></pre></td></tr></tbody></table></figure></li></ol><h2 id="插件安装"><a href="#插件安装" class="headerlink" title="插件安装"></a>插件安装</h2><h3 id="mermaid流程图"><a href="#mermaid流程图" class="headerlink" title="mermaid流程图"></a>mermaid流程图</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 安装插件</span></span><br><span class="line">npm install hexo-filter-mermaid-diagrams</span><br><span class="line"></span><br><span class="line"><span class="comment">## 在当前使用的主题的.config.yml中开启功能， ./themes/.config.yml</span></span><br><span class="line">mermaid:</span><br><span class="line">  on: <span class="literal">true</span></span><br></pre></td></tr></tbody></table></figure><h1 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h1><p><a href="https://yelog.org/2017/03/13/3-hexo-homepage/" target="_blank" rel="noopener">3-hexo-homepage</a><br><a href="https://theme-next.iissnan.com/theme-settings.html" target="_blank" rel="noopener">next</a><br><a href="https://vic.kim/2019/05/25/Hexo%E5%8D%9A%E5%AE%A2%E4%BC%98%E5%8C%96%E4%B9%8BNext%E4%B8%BB%E9%A2%98%E7%BE%8E%E5%8C%96/" target="_blank" rel="noopener">next主题美化</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bolza问题, Legendre变换，从最速降线说起</title>
      <link href="/2022/03/01/bolza/"/>
      <url>/2022/03/01/bolza/</url>
      
        <content type="html"><![CDATA[<h1 id="Bolza问题"><a href="#Bolza问题" class="headerlink" title="Bolza问题"></a>Bolza问题</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>最近在学习The financial mathematics of market liquidity 和游戏中物理模拟的一些知识，发现这两个地方都涉及到了Bolza问题，决定单独把这块提取出来说一说。我们这边主要把Bolza问题集中在最小化这个小问题上面:</p><script type="math/tex; mode=display">J = \int_{0}^T f_t(x(t)) + g_t(\dot{x}(t))dt</script><p>并且满足边界条件$x(0) = a, x(T) = b$。<br><a id="more"></a></p><p>当然更加一般的Bolza问题可以参考这个链接<br><a href="https://encyclopediaofmath.org/wiki/Bolza_problem" target="_blank" rel="noopener">https://encyclopediaofmath.org/wiki/Bolza_problem</a> 这边我们考虑是其简化版本，在一般的兼容建模和物理模拟中足够使用了。</p><blockquote><p>NOTE<br>笔者背景偏理论物理向，所以整个Blog在数学上可能不是那么严谨，但求能够讲清楚其背后的一些原理性的东西</p></blockquote><h2 id="最速降线"><a href="#最速降线" class="headerlink" title="最速降线"></a>最速降线</h2><p>看到上面那个带优化的方程，对于学过一些物理的朋友来说，第一反应就是联想到Lagrangian。那么这边我们就从经典的最速降线说起。最速降线要解决的一个问题就是，在纯重力场下面，找到一条轨道，使得从点A到点B(点A高度高于点B)，用的时间最短。如图所示<br><img src="./bolza/desc.jpg" alt=""><br>显然，我们要优化的东西是 $T=\int_0^T dt$，通过简单的运动学和能量守恒，我们就可以得到，</p><script type="math/tex; mode=display">dt = \frac{\sqrt{dx^2 + dy^2}}{v} = \frac{\sqrt{1+y'^2(x)}}{\sqrt{2gy}}dx</script><p>因此我们最终的优化问题是：</p><script type="math/tex; mode=display">J[y(x)] = \int_{x_A}^{x_B}\frac{\sqrt{1+y'^2(x)}}{\sqrt{2gy}}dx</script><p>从这边我们可以看出来，最速降线就是一种Bloza问题～</p><h2 id="Legendre变换"><a href="#Legendre变换" class="headerlink" title="Legendre变换"></a>Legendre变换</h2><p>提到Legendre变换，我想对于物理系毕业的学生，一下就会想到理论力学和热力学学到的两个场景</p><h3 id="理论力学"><a href="#理论力学" class="headerlink" title="理论力学"></a>理论力学</h3><p>众所周知，理论力学有两套描述方式，一套走的拉格朗日力学，一套走的哈密顿力学。从广义坐标和虚功原理出发，我们可以很容易找到在保守场中动力学描述：</p><script type="math/tex; mode=display">L = T(q,\dot{q})- V(q)$$，这边$q, T, V$分别代表系统的广义坐标，动能，保守势能。动力学对应的微分方程是</script><p>\frac{d}{dt}\frac{\partial L}{\partial \dot{q}_k} = \frac{\partial L}{\partial q_k}</p><script type="math/tex; mode=display">哈密顿力学则可以通过Legendre变换，从拉格朗日力学中转变而来，</script><p>\dot{p}<em>k = -\nabla</em>{q<em>k}H(p,q), \dot{q}_k = \nabla</em>{p_k}H(p,q)</p><script type="math/tex; mode=display">这边动量$p:=\frac{\partial L}{\partial \dot{q}_k}$，而哈密顿量$H$则是拉格朗日量通过Legendre变换得来 $H = p^T \dot{q} - L(q, \dot{q})$。### 热力学在热力学中，我们知道有几个基本的物理量，焓$H$, 哪能$U$, 亥姆霍兹自由能$F$，吉布斯自由能$G$，满足下面的关系</script><p>H = U + PV,<br>G = H - TS,<br>A = U - TS</p><script type="math/tex; mode=display">这边$T,S$ 代表系统的温度和熵，$P,V$代表系统的压强和体积。笔者，当初在学习热力学的时候，对于$(T,S), (P,V)$这两组共轭变量，也是只是认为量纲互补，也是最近才理解了这边共轭的来源，惭愧...### 数学数学上对于Legendre变换的定义如下：> 定义> 如果函数$f(x): R^d \to R$ 在我们所关心的区域内是可导的，函数$f(x)$对应的Legendre变换定义为> $$ f^*(p) = p^T x - f(x) |_{\frac{d(px-f(x))}{dx} = 0}</script><p>从图上我们可以看到Legendre变换的几何意义:<br><img src="./bolza/leg.png" alt=""><br>从这定义，我们可以看出来，加入$f(x)$一个满足超线性$ \lim_{|x|\to\infty}\frac{|f(x)|}{|x|} = \infty$的凸函数，那么这个定义等价于</p><script type="math/tex; mode=display">f^*(p) = \sup_{x} p^T x - f(x)</script><p>对着演示的这幅图，我们可以想象，比如$x$越来越大的时候，$f(x)$增加越来越快，对应的切线变陡峭的速率也越来越快，$f^*$ 增加得也越来越快。因此，我们可以claim，数学证明也是很简单的，在这边就不赘述了。</p><blockquote><p>性质<br>$f^*$也是一个凸函数</p></blockquote><p>对比上面两种物理，尤其是理论力学中的结构，我们可以任何一个拉格朗日量，对应的哈密顿量也会继承其的凹凸性，这点是非常重要有意思的，有兴趣的读者可以脑子里想象一下简单的弹簧系统。</p><!-- 那么对于那些不可导的凸函数，我们可以定义一个```subgradient```来代替提督> 定义> 假设$f:C \to R$是一个凸函数，对于定义域内的一点$x$，这点的```subgradient```是定义为满足下面条件的集合:> $$ \{ p | \forall y \in C, f(y) \ge f(x) + p(y - x) \}$$ --><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 理论物理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 量子物理，机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《刚性泡沫》读书笔记</title>
      <link href="/2021/01/01/%E5%88%9A%E6%80%A7%E6%B3%A1%E6%B2%AB/"/>
      <url>/2021/01/01/%E5%88%9A%E6%80%A7%E6%B3%A1%E6%B2%AB/</url>
      
        <content type="html"><![CDATA[<h1 id="刚性泡沫">刚性泡沫</h1><p>全书描述了中国经济遇到困境，并认为政府的隐性担保和刚性兑付难辞其咎，同时强调市场自由的重要性。<a id="more"></a></p><h2 id="信用流">信用流</h2><pre class="mermaid">graph TDG1[中央政府] --&gt; G2[地方政府]G2 --保证房价不跌--&gt; G3[房地产]G2 --保证收益稳定--&gt; G4[影子银行]G1 --&gt; G5[证监会]G5 --保证股市繁荣 --&gt; G6[股市]G1 --政策优惠--&gt; G7[国有企业]G4 --&gt; G8[信托产品]G4 --&gt; G9[委托存款]</pre><p>备注：</p><ul><li>通过发行收益丰厚同时被认为非常安全的信托产品和理财计划，把资金贷给房地产开发商和地方政府融资平台，这些传统银行不能放贷的领域。</li><li>委托贷款，一个公司把自己的资金贷给另一个公司，影子银行在其中扮演中介。</li></ul><p>为什么说中国存在刚性兑付：</p><ul><li>国际的证监会职责只有一条，保证市场上的信息披露公正，公平，公开。而中国证监会一直在为股市提供隐形担保，这实际造成了中国股市上的赌博行为更加明显，整个股市长期处于高波动状态。</li><li>国有企业享有特别的政策优惠，例如垄断行业的准入权，重点核心项目的参与权，政府采购，退税及其他财政优惠。因此导致了中国国有企业的负债过高，产能过剩的明显问题。</li></ul><h3 id="影子银行">影子银行</h3><p>发达国家金融体系:</p><ul><li>股票</li><li>债券</li></ul><p>中国金融体系</p><ul><li>银行</li></ul><p>为什么影子银行在中国规模如此之大：</p><ul><li>金融压抑，实体经济对资金的渴求</li><li>资本金充足率的要求<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a><ul><li>影子银行可以帮助银行，将表内业务转至表外，从而削减资产规模，达到降低自有资本金的目的。</li></ul></li><li>存贷款比率和存款准备金的要求<ul><li>通过影子银行把不能算作银行普通存款的银行间存款转成普通存款，从而提高存款规模，降低存贷款比率。</li></ul></li></ul><h3 id="房地产">房地产</h3><p>中国房地产存在严重的泡沫，并且通过数据分析，排除了是刚需的动机。</p><ul><li>房价上涨速度</li><li>租金和房价比例</li><li>房价和居民收入比例</li><li>土地总价值</li></ul><p><code>传统银行</code>，<code>影子银行</code>和<code>地方融资平台</code>很多资金都进入了<code>房地产</code>。这些金融机构都因房地产面临巨大的风险敞口。</p><h3 id="中国股市">中国股市</h3><p>中国股市缺乏抑制泡沫增长的工具</p><ul><li>做空机制</li><li>衍生品种类太过单一</li><li>外汇限制，导致了哄抬国内资产价格</li></ul><p>A股严厉的审批制度带来一下的缺点：</p><ul><li>很多真正的好公司并不能在A股上市</li><li>严厉的审批，会误导投资者，认为能在A股上市的公司都是好公司，降低了投资者的判断能力</li><li>严厉的审批，造成了A股公司普遍的高估值</li><li>严厉的审批，将投资风险部分转移到证监会，这一中性机构，相当于提供了隐形担保。</li><li>没有完备的退市机制，导致上市公司融资之后，并不扩大主营业务，而是投入到房地产等领域，扩大了整个系统风险。</li></ul><h3 id="中国的金融创新">中国的金融创新</h3><p>中国特色的金融产品:</p><ul><li>余额宝之类的互联网金融产品<ul><li>网民数量之大，人均理财数额不大，提供了充足的流动性</li><li>中国网民缺乏理财常识，并不能意识到其中的风险</li><li>互联网企业通过做大自己，吸引政府关注，将该类投资品的风险转加给政府，让政府为其提供隐形担保和刚性兑付。</li></ul></li><li>P2P</li><li>大宗商品融资<ul><li>套利机制，依赖于大宗商品的价格稳定</li></ul></li></ul><h3 id="中国经济动力">中国经济动力</h3><p>三驾马车</p><ul><li>出口<ul><li>劳动力优势减弱</li><li>企业生产效率低下</li></ul></li><li>内需<ul><li>中国社会文化导致人们对奢饰品的追求，并不能促进国内的消费需求</li><li>日益高涨的房价，挤压年轻人的消费需求</li><li>高度不确定的社会保障体制，提高了人们的存款需求</li></ul></li><li>投资<ul><li>中国企业的投资回报率在下降</li></ul></li></ul><h3 id="产能过剩">产能过剩</h3><p>中国经济面临产能过剩的主要行业</p><ul><li>钢铁，水泥，电解铝，平板玻璃，造船</li><li>汽车行业</li><li>新能源行业：光伏，风电</li></ul><p>产能过剩的三个原因</p><ul><li>中央政府和地方政府责任不可推卸<ul><li>中央政府在规划中，点名了某些行业，就会导致地方政府过分实践，从而资金资源大量涌入，立马导致行业的产能过剩</li></ul></li><li>企业本身不可推卸<ul><li>企业乐观认为中国经济的高速增长会一直持续，因此疯狂扩张</li><li>疯狂的扩张，期望获得政府的隐形担保</li></ul></li><li>金融制度的不完善<ul><li>破产制度，缺乏合理的退出制度</li><li>只要国内的利率水平和人民币汇率还受到管制，那么市场将无法最大限度地发挥其配置资源的功能</li></ul></li></ul><h3 id="国有企业概貌">国有企业概貌</h3><p>中国国有企业规模宏大的原因(现在似乎互联网公司已经超过这些国有企业了)</p><ul><li>宏观经济因素，中国经济大盘发展很好</li><li>国家政策的导向</li></ul><p>国有企业的优势</p><ul><li>业务规模大</li><li>品牌相对民企较硬</li><li>行业具有垄断特性</li></ul><p>国有企业的劣势</p><ul><li>实现的目标多元化，不能利润最大化，提升运行效率</li><li>政府和国有企业关联太大，仕途上升比企业利润最大化更要</li><li>中央计划者很难收集国有企业真实的运营信息</li><li>因为一股独大，公司治理缺失</li><li>负债过高，杠杆过高</li></ul><p>对比上面，国有的劣势，可以想像到国有企业改革方向有</p><ul><li>削减债务，去杠杆</li><li>利率化市场改革，增加国企的融资难度，促进投资的效率，也可以去杠杆</li><li>放松行业管制和政策限制，让民企更多参与到与国企的竞争中</li><li>完善国企信息披露制度和考评制度</li><li>法制改革，划清政府和市场的界限</li></ul><h3 id="巫术统计学">巫术统计学</h3><p>中国官方的统计数据灵活性很大。<code>研究一项数据指标，必须要明白指标的具体计算方式</code></p><h3 id="总结">总结</h3><p>中国经济取得成功的原因</p><ul><li>改革开放等制度的保证</li><li>政府对流动性的注入的保证</li><li>政府对投资收益的保证</li></ul><p>中国经济走出困境的方式</p><ul><li>市场在资源配置中起决定性作用</li><li>调整政府和市场的关系</li><li>加强法制建设，实现“买者自负，卖者有责”</li><li>媒体监督政府和市场的关系</li><li>政府退出<ul><li>降低货币供应量</li><li>利率市场化</li><li>资本项下放</li><li>退出房价保障</li><li>退出股票市场保障</li><li>加强影子银行，P2P监管</li><li>中央政府退出对地方政府的债务担保</li></ul></li></ul><h2 id="摘要">摘要</h2><ul><li>如果企业的经营者预计货币供应会越拉越宽松，那么一定可以预测到，资产价格出现明显的上涨。在对国家的通胀水平和资产价格有如此的预期之下，企业经营者合乎逻辑的做法就是大规模借入债务和扩张自己的资产和产能，以期待在下一轮的泡沫中获利。很多开始被认为是非常激进的投资举措，随着泡沫的快速扩张，事后被认为是非常明知的，甚至是非常神圣的。这种成功押宝的经济扩张和资产价格升值的经验，导致企业家越来越有信心进行尽可能的固定资产投资和产能扩张。</li><li>破产是对资源的重新配置</li><li>破产本身最有价值的一点，就是让市场决定哪些企业可以存活哪些不可以，哪些资产还有价值哪些没有</li><li>死亡是生命最伟大的发明（乔布斯）</li></ul><h1 id="脚注">脚注</h1><aside id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr><ol><li id="fn1"><p>巴塞尔协议要求银行的自有基本金必须达到总资产规模的百分之八。<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></aside><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 投资 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 宏观投资 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Trace, off-policy, on-policy</title>
      <link href="/2020/06/30/trace/"/>
      <url>/2020/06/30/trace/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要">摘要</h2><p>在这篇blog中，我们会介绍几个trace技巧，同时介绍研究人员如何用trace来把off-policy的数据纠正来计算关于on-policy的objective。<a id="more"></a></p><h2 id="tdlambda-和-eligibility-traces-4">TD(<span class="math inline">\(\lambda\)</span>) 和 Eligibility Traces [^4]</h2><!-- ### 简介 --><p>众所周知，在强化学习中，我们需要估计valuefunction，也就是要计算下面<span class="math inline">\(G_t\)</span>的数学期望。 <span class="math display">\[G_t = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{T-t-1}R_T\]</span> 那么很自然的有两种流派：</p><ul><li>第一种就是每次完备地收集<span class="math inline">\(R_{t+1},\cdots,R_T\)</span>这些数据，然后不断重复，最后计算出一个samplemean，这就是完全的基于return的方式。因为完备地获取数据需要不断的蒙特卡罗模拟整个过程，所以也称之蒙特卡罗方法。</li><li>第二种就是我们只模拟到某一步，剩下的部分用一个函数去近似，然后整个过程bootstrap。在最极端的情况下，就是只模拟一步，剩下全部用函数来近似:<span class="math inline">\(G_t=R_{t+1}+\gamma V(S_{t+1})\)</span></li></ul><p>当然这边我们考虑一个比较general的做法，就是模拟 <span class="math inline">\(n\)</span> 步。我们可以定义<span class="math inline">\(n\)</span>步return的概念， <span class="math display">\[G_t^{t+n}=R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1}R_{t+n}+\gamma^nV(S_{t+n}) \tag{1}\]</span> 即所谓的中庸之道。当然因为<span class="math inline">\(n\)</span>步的return在实际操作中的花费会比较大，因为每次都需要向前模拟<span class="math inline">\(n\)</span>步。因此人们提出来了<span class="math inline">\(TD(\lambda)\)</span>。</p><h3 id="tdlambda"><span class="math inline">\(TD(\lambda)\)</span></h3><p>在方程.(1)的基础上，做进一步的改进，考虑一种特殊的线性结合的方式结合起来所有的<span class="math inline">\(n\)</span>步return，例如， <span class="math display">\[L_t(\lambda) = (1-\lambda)\sum_{n=1}^{+\infty}\lambda^{n-1}G_{t}^{t+n}\]</span> 对于在有限步<span class="math inline">\(T\)</span>能够结束的游戏，我们做一个截断， <span class="math display">\[L_t(\lambda) = (1-\lambda)\sum_{n=1}^{T-t-1}\lambda^{n-1}G_{t}^{t+n} +\lambda^{T-t-1}G_t.\]</span> 通常意义上，人们称这种定义下的return为<span class="math inline">\(\lambda\)</span>-return</p><h3 id="eligibility-trace">Eligibility Trace</h3><p>从<span class="math inline">\(\lambda\)</span>-return的定义，似乎比<span class="math inline">\(n\)</span>步的return更加复杂，因为我们需要模拟出所有的游戏步骤。幸运的是，聪明的研究人员证明了下面的计算方式等价于上面的定义。为了更好的表达，我们引入Eligibilitytrace通过定义其更新方式: <span class="math display">\[E_{t}(s) = \gamma\lambda E_{t-1}(s) + I_{s=S_t}\]</span> 在这些符号下，那么每步更新只需要TD error和eligibility trace，<span class="math display">\[\Delta V_t(s) = \alpha \delta_t E_t(s), \delta_t = R_{t+1} + \gammaV_{t}(S_{t+1}) - V_t(S_t)。\]</span>这边我们不做严格的数学证明。仅仅给出一个直觉上理解方式。让我们首先忘记强化学习这件事，考虑一个简单的case，我们想要估计一个硬币正面向上的数学期望通过不断抛硬币。假设在抛了<span class="math inline">\(n\)</span>次之后，我们的估计结果是<span class="math inline">\(p_n\)</span>，然后我们再抛一次，试图去更新估计。最trivial的方式就是，<span class="math display">\[p_{n+1} = (1-\lambda)p_n + \lambdaI_{up}\]</span> 这边<span class="math inline">\(I_{up}\)</span>意味如果第<span class="math inline">\(n+1\)</span>结果是向上，那么取1，否则取0。这不是和上面的过程有某种类似？现在我们思考eligibilitytrace做了什么，那么就比较简单了。其实就对于采样到的结果有了一个click，这边就是采取的加1的方式，然后随着时间的流失，这些结果的影响加上了discount。这边需要强调的discount并不具有真正意义时间序的关系，更多的是为了满足normalization，这样可以保证估计子的consistency。</p><h2 id="使用各种trace来链接off-policy和on-policy">使用各种trace来链接off-policy和on-policy</h2><h3 id="summary">Summary</h3><p>在这个部分，我会考虑如何使用trace的技巧来弥补off-policy和on-policy的差距。比如我们的数据是从策略<span class="math inline">\(\mu\)</span>中而来，但我们的目标是要估计<span class="math inline">\(Q^{\pi}\)</span>而非<span class="math inline">\(Q^{\mu}\)</span>,那我们需要怎么做呢。最简单的和我们之前所讨论一样使用importancesampling。假设我们有一个比较general的操作， <span class="math display">\[\mathcal{R}Q(s,a):=Q(s,a)+E_{\mu}\left[\sum_{t\ge 0}\gamma^t(\prod_{s=1}^tc_s)(r_t+\gamma E_{\pi}Q(s_{t+1},\cdot)-Q(s_t,a_t))\right]\]</span></p><ul><li>Importance Sampling [^3] (<span class="math inline">\(c_s=\frac{\pi(a_s|s_s)}{\mu(a_s|s_s)}\)</span>)显然我们可以纠正off-policy，但是这种做法有个问题，就是在一般情况下，<span class="math inline">\(\frac{\pi(a_s|s_s)}{\mu(a_s|s_s)}\)</span>有很高的方差，就会导致算法很不稳定。</li><li>Off-policy <span class="math inline">\(Q^{\pi}(\lambda),Q^{*}(\lambda)\)</span> [^5] (<span class="math inline">\(c_s=\lambda\)</span>) 这种trace需要策略<span class="math inline">\(\mu\)</span>不是那么off-policy。量化来讲，如果<span class="math inline">\(\epsilon=\max_s\|\pi(\cdot|s)-\mu(\cdot|s)\|_1\)</span>，为了保证<span class="math inline">\(Q^{\pi}(\lambda)\)</span>收缩性质，<span class="math inline">\(\lambda &lt;\frac{1-\gamma}{\gamma\epsilon}\)</span>; 为了保证<span class="math inline">\(Q^{*}(\lambda)\)</span>的收缩性质，<span class="math inline">\(\lambda &lt;\frac{1-\gamma}{2\gamma}\)</span>。</li><li>Tree-backup, TB(<span class="math inline">\(\lambda\)</span>) <span class="math inline">\((c_s=\lambda\pi(a_s|s_s))\)</span>这种方式可以保证任意的off-policy都能收敛，但是很明显，如果<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\pi\)</span>很接近，那么效率就会很低，因为<span class="math inline">\(c_s\)</span>一直discount采样的效率，而在这种情况适合的<span class="math inline">\(c_s\)</span>应该在<span class="math inline">\(1\)</span>附近。</li><li>Retrace(<span class="math inline">\(\lambda\)</span>)[^1] (<span class="math inline">\(c_s=\lambda \min\left(1,\frac{\pi(a_s|s_s)}{\mu(a_s|s_s)}\right)\)</span>)这个方法就是综合了上面三个方法的特点。<ul><li>给了trace一个bound，限制了方差</li><li>on-policy的时候，没有一直discount trace</li><li>off-policy的时候，限制了trace。</li></ul></li><li>V-trace [^2] (<span class="math inline">\(c_s=\min(\lambda,\frac{\pi(a_s|s_s)}{\mu(a_s|s_s)})\)</span>)</li></ul><p>V-trace除了用了一个不同的trace，还直接把temporaldifference做了纠正，即映射用的是， <span class="math display">\[\mathcal{R}Q(s,a):=Q(s,a)+E_{\mu}\left[\sum_{t\ge 0}\gamma^t(\prod_{s=1}^tc_s)\rho_t(r_t+\gammaE_{\pi}Q(s_{t+1},\cdot)-Q(s_t,a_t))\right],\]</span> 这边<span class="math inline">\(\rho_t\)</span>是一个被截断IS纠正，<span class="math inline">\(\rho_t=\min(\bar{\rho},\frac{\pi(a_t|s_t)}{\mu(a_t|s_t)})\)</span>并且<span class="math inline">\(\bar{\rho}\ge\lambda \ge1\)</span>。作者声称的一个好处是在完全on-policy的时候，V-trace可以完全复原也就是<span class="math inline">\(\rho_t=c_s=1\)</span>，但是Retrace不可以<span class="math inline">\(c_s=\lambda\)</span>。</p><h3 id="数学定理">数学定理</h3><p>下面是两个重要的数学定理，从理论的角度保证了这些trace技巧的可行性。[^1]</p><h4 id="theorem-1">Theorem 1</h4><p>The operator <span class="math inline">\(\mathcal{R}\)</span> has auniqued fixed point <span class="math inline">\(Q^{\pi}\)</span>. If foreach <span class="math inline">\(a_s\)</span> and history <span class="math inline">\(\mathcal{F}_s\)</span> we have <span class="math inline">\(c_s=c_s(a_s,\mathcal{F}_s) \in [0,\frac{\pi(a_s|s_s)}{\mu(a_s|s_s)}]\)</span>, then for any function <span class="math inline">\(Q\)</span>, <span class="math display">\[\|\mathcal{R}Q-Q^{\pi}\| \le \gamma \|Q-Q^{\pi}\|\]</span></p><h4 id="definition">Definition</h4><p>We say that a sequence of policies <span class="math inline">\(\pi\)</span> is increasingly greedy a sequence<span class="math inline">\(Q_k\)</span> of Q functions if the followingproperty holds for all <span class="math inline">\(k\)</span>: <span class="math inline">\(P^{\pi_{k+1}}Q_{k+1} \geP^{\pi_k}Q_{k+1}\)</span>, where operator <span class="math inline">\(P\)</span> is defined as, <span class="math display">\[P^{\pi}Q(s,a) =\sum_{s',a'}\pi(a'|s')p(s'|s,a)Q(s',a')\]</span></p><h4 id="thoerem-2">Thoerem 2</h4><p>Consider an arbitrary sequence of behaviour policies <span class="math inline">\(\mu_k\)</span> and a sequence of target policies<span class="math inline">\(\pi_k\)</span> that are increasingly greedyw.r.t. the sequence <span class="math inline">\(Q_k\)</span>: <span class="math display">\[Q_{k+1} = \mathcal{R}_k Q_k\]</span> where <span class="math inline">\(\mathcal{R}_k\)</span> isfor <span class="math inline">\(\pi_k, \mu_k\)</span> and a<strong>Markovian</strong> <span class="math inline">\(c_s=c(a_s,s_s)\in[0, \frac{\pi(a_s|s_s)}{\mu(a_s|s_s)}]\)</span>. Assume the targetpolicies <span class="math inline">\(\pi_k\)</span> are <span class="math inline">\(\epsilon_k\)</span>-away from the greedy policiesw.r.t. <span class="math inline">\(Q_k\)</span>, in the sense that <span class="math inline">\(\mathcal{T^{\pi_k}}Q_k \ge \mathcal{T}Q_k-\epsilon_k\|Q_k\|e\)</span>, where <span class="math inline">\(e\)</span> is the vector with <span class="math inline">\(1\)</span>-components, and <span class="math display">\[\mathcal{T}^{\pi}Q:=r+\gamma P^{\pi}Q, \mathcal{T}Q := r + \gamma\max_{\pi}P^{\pi}Q\]</span> Furthermore suppose <span class="math inline">\(\mathcal{T}^{\pi_0}Q_0 \ge Q_0\)</span>. Then forany <span class="math inline">\(k\ge0\)</span>, <span class="math display">\[\|Q_{k+1}-Q^{*}\| \le \gamma \|Q_k -Q^*\| + \epsilon_k \|Q_k\|\]</span> In consquence, if <span class="math inline">\(\epsilon_k \to0\)</span>, then <span class="math inline">\(Q_k \to Q^*\)</span>.</p><h2 id="reference">Reference</h2><p>[^1] Munos, Rémi, et al. "Safe and efficient off-policy reinforcementlearning." Advances in Neural Information Processing Systems. 2016. [^2]Espeholt, Lasse, et al. "Impala: Scalable distributed deep-rl withimportance weighted actor-learner architectures." arXiv preprintarXiv:1802.01561 (2018). [^3] Precup, Doina. "Eligibility traces foroff-policy policy evaluation." Computer Science Department FacultyPublication Series (2000): 80. [^4] Sutton, Richard S., and Andrew G.Barto. Introduction to reinforcement learning. Vol. 135. Cambridge: MITpress, 1998. APA [^5] Harutyunyan, Anna, et al. "Q (<span class="math display">\[\lambda \]</span>) with Off-Policy Corrections."International Conference on Algorithmic Learning Theory. Springer, Cham,2016. APA</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>三个分布式RL算法</title>
      <link href="/2020/06/29/disoptimzer/"/>
      <url>/2020/06/29/disoptimzer/</url>
      
        <content type="html"><![CDATA[<p>简单介绍一下三个分布式算法 <a id="more"></a> ## Apex(Distributedprioritized experience replay)</p><h3 id="per">PER</h3><p>在介绍分布式的PER之前，先简单介绍一下原始版本的PER[^1].PER的提出主要是为了解决如何提高数据利用效率的问题。比方说，一个学生在学习数学的时候，他需要去做题来提高他的理解认知能力。很简单的一个问题就是，学生需要做哪些题目才能更快地提高他的数学能力。直觉上，我们认为去练习那些原来不会做的题目比一直练习已经会做的题目可以更有效率地提高数学水平。PER则是利用来这一点，通过提高和当前认知相差较远的数据（譬如loss很大）的采样概率，降低和当前认知相近的数据的采样概率，来增强算法的学习能力。</p><p>具体来讲，在PER中，适应来TDerror来衡量能够学习到的内容。直觉上认为从TDerror越大的数据中，我们可以学习到更多有效的信息。同时为了提高鲁棒性，并没有直接使用hard的贪心算法，而是引入一个超参数<span class="math inline">\(\alpha\)</span>来调节采样的贪心程度，例如， <span class="math display">\[P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}\]</span> 对于 <span class="math inline">\(p_i\)</span>的选取，提出了两种选择方法 <span class="math inline">\(p_i=|\delta_i|+\epsilon\)</span> 或者 <span class="math inline">\(p_i =\frac{1}{\mathrm{rank}(|\delta_i|)}\)</span>.(在实现的时候，新收集的数据因为没有TDerror，所以会给一个很大的priority，保证能够被选中)。如果我们仔细检查这个过程会发现，有一个严重的问题，就是认为的改变了采样概率，那么最终我们算出来的所有期望值并不会是我们真正想要得到的值。对于这种情况，我们需要引入IS(importancesampling)。 ### Importance Sampling IS本质是一个很简单的技巧。比如我们想要计算随机变量<span class="math inline">\(f(X)\)</span>在分布<span class="math inline">\(p(x)\)</span>下的期望值<span class="math inline">\(E_p[f(X)]=\int f(x)p(x)dx\)</span>.但是由于某些原因，我们获得的数据是在分布<span class="math inline">\(q(x)\)</span>下获得的，那么我可以做一个简单的变型，通过<span class="math inline">\(q(x)\)</span>下的数据依然可以计算出<span class="math inline">\(E_p[f(x)]\)</span>。 <span class="math display">\[E_p[f(X)] = \int f(x)p(x)dx = \int [f(x)\frac{p(x)}{q(x)}]q(x) dx\]</span> 也就是说我们只需要给原始的随机变量<span class="math inline">\(f(X)\)</span>一个权重<span class="math inline">\(\frac{p(X)}{q(X)}\)</span>即可。</p><p>在实际操作过程中，引入一个超参数<span class="math inline">\(\beta\)</span>来控制这个权重，例如， <span class="math display">\[w_i = \left(\frac{1}{N}\frac{1}{P(i)}\right)^{\beta}\]</span> 为了保证程序的稳定性，通常会乘<span class="math inline">\(\frac{1}{\max_iw_i}\)</span>归一化这个权重。显然如果<span class="math inline">\(\beta=1\)</span>，那么这个权重可以完全纠正采样分布的偏颇。因此就存一个在priority和correctness之间存在一个tradeoff。在实践过程中，往往通过初始化一个<span class="math inline">\(\beta_0\)</span> 然后慢慢退火到 <span class="math inline">\(1\)</span>.还有一点需要注意的就是IS也会让整个随机梯度下降变得更加的稳定。因为给了大的梯度一个discount，这样更加满足梯度下降所需要的locality。</p><h3 id="apex">Apex</h3><p>Apex算法则是把上述算法拓展成分布式的结构，如下图。 <img src="apex.png"></p><ul><li><p>Learner 从 Replay之间的采样与反馈加入了priority。</p></li><li><p>相比与单机式的算法，在分布式结构中每个actor需要自己计算出一个初始的priority给新加入的数据。否则如果和单机式的算法一样，给新的数据一个很大的priority的话，会有出现问题。因为在分布式的结构中，我们有很多的actor，那么如果所有的priority都有learner来更新维护，那么learner在每次采样的时候都会选取最近的data。</p></li><li><p>共享数据比共享梯度对于off-policy来讲更加鲁棒。</p></li><li><p>因为分布式的actor，每个都可以探索不同的领域，这样能够更有效解决agent的explore-exploittrade-off.</p></li></ul><p>Apex-DQN 对一般的DQN做了一些修改，使用了DoubleQ和n-step，具体来说，<span class="math display">\[G_t = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^nq(S_{t+n}, \arg\max_a q(S_{t+n},a,\theta), \theta^-)\]</span> Apex-DDPG <span class="math display">\[G_t = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^nq(S_{t+n}, \pi(S_{t+n},\psi^-), \theta^-).\]</span></p><h2 id="impala">IMPALA</h2><p>IMPALA是<strong>Imp</strong>ortance Weighted<strong>A</strong>ctor-<strong>L</strong>earner<strong>A</strong>rchitecture的缩写。顾名思义，IMPALA框架下，最重要的两个点：</p><ul><li>Importance weight: V-trace</li><li>Actor-Learner 架构：异步</li></ul><h3 id="v-trace">V-trace</h3><p>V-trace 可以参考另一篇blog <a href="/2020/06/30/trace/" title="Trace, off-policy, on-policy">Trace, off-policy, on-policy</a></p><h3 id="actor-learner">Actor-Learner</h3><p><img src="impala.png">正如上图所示，左边展示的拥有多个actor，一个learner，那么每个actor都把自己采样的数据传给同一个learner，learner使用GPU更新参数。当这种框架扩展到多个actor，多个learner的时候，就如右图所示。一个learner周围有很多actor，比如一个GPU和多个CPU协同，这些actor把自己采样到的数据发送属于自己的learner上去，但是在反向同步参数的时候，actor并不需要一定是从自己对应的learner上去获取参数，而是快速从任意learner获取参数。</p><h2 id="参考文献">参考文献</h2><p>[^1]Schaul, Tom, et al. "Prioritized experience replay." arXivpreprint arXiv:1511.05952 (2015).</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>量子傅立叶变换</title>
      <link href="/2020/06/18/qft/"/>
      <url>/2020/06/18/qft/</url>
      
        <content type="html"><![CDATA[<p>经典的傅立叶变换，无论对工程应用还是数据科学，都产生了深远。因此对于量子计算机来说，如何实施傅立叶变换也是很重要的事情<sup><a href="#fn_1" id="reffn_1">1</a></sup>。<br><a id="more"></a><br>离散傅立叶变换，是将一组振幅 ${x_j}$ ，通过如下的变换，转换成另一组共轭的振幅 ${\tilde{x}_k}$ ,</p><script type="math/tex; mode=display">\tilde{x}_k = \frac{1}{\sqrt{N}}\sum_{j=1}^{N}e^{i\frac{2j\pi}{N}k}x_j \tag{1}</script><p>对于量子计算机而言，这些都是编码的概率幅，因此，我们只要对computational basis做傅立叶变换，例如：</p><script type="math/tex; mode=display">|\tilde{k}\rangle = \frac{1}{\sqrt{N}}\sum_{j=1}^{N}e^{i\frac{2\pi j}{N}k}|j\rangle \tag{2}</script><p>比较容易观察到，这组变换是unitary的。既然如此，我们就希望找到一种物理实现。<br>在谈具体实现之前，我们可以直觉上思考一些事情。1）对于一个n-qbit的量子计算机， $N=2^n ， j=j<em>{n-1}j</em>{n-2}\cdots j<em>0 $，后者我们用了二进制串。2）同时我们需要意识到，以 $2\pi$ 为单位，每次转动的角度是 $\frac{1}{2^n}$ ，对于所有的 $j$ ，正好覆盖了所有$n$比特长度的小数， $\sum</em>{k=0}^{n-1}j_k2^{k-n}$ ，如果我们读取了量子态的相位，那么可以很好的知道 $k$ ，这为后面的很多算法做了奠基工作。<br>基于上面的我们可以给出Eq.(2)的等价描述，</p><script type="math/tex; mode=display">|j_{n-1}j_{n-2}\cdots j_0\rangle \rightarrow \frac{1}{\sqrt{2^n}}\bigotimes_{l=0}^{n-1}(|0\rangle_l + e^{2\pi i 0.j_{n-l-1}\cdots j_0}|1\rangle_l)</script><p>从这个表达式，我们可以看出来，变换之后的态仍然是可分的，那么我们只需要关注如何把一个Qbit从 $|j<em>{n-l-1}\rangle  变成 |0\rangle+e^{2\pi i 0.j</em>{n-l-1}\cdots j<em>0}|1\rangle$。首先相位 $e^{2\pi i 0.j</em>{n-l-1}}$ 通过一个Hadamard，直接可以得到。从第二位开始，相位的值依赖于其他的Qbit，因此需要借助控制门才能完成。例如我们需要把 $|0\rangle \rightarrow \frac{|0\rangle + e^{2\pi i 0.0b}|1\rangle}{\sqrt{2}}$（注意这边的小数仍然一个二进制串），那么我可以加一个基于b的值的控制门，来完成：如果 $b=0$ ，不执行操作；如果 $b=1$ ，执行一个门 $\left(\begin{array}{cc}1 &amp; 0 \ 0 &amp; e^{2\pi i 2^{-2}} \end{array}\right)$ . 因此对于 $l$ 位的比特来说，我们也可以加基于所有他之后所有比特的控制门 $\left(\begin{array}{cc} 1 &amp; 0 \ 0 &amp; e^{2\pi i 2^{-d}} \end{array}\right)$，这里的 $d$ 代表距离 $l$ 的位数。</p><p>复杂度： $O(n^2)$<br>我们需要做 $\frac{n(n+1)}{2}$ 次门操作。<br>备注：概率幅并不如经典的振幅那样容易获得，因此直接应用QFT，不是那么容易，尽管我们展示了指数加速的能力。</p><p><sup><a href="#fn_1" id="reffn_1">1</a></sup>:Quantum Computation and  Quantum Information, Michael A.Nielsen, Issac L. Chuang</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 理论物理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 量子计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ray 框架简介</title>
      <link href="/2020/06/01/ray/"/>
      <url>/2020/06/01/ray/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Ray是一个款UCB研究人员设计的分布式机器学习框架，尤其是针对强化学习。本文第一部分介绍分布式机器学习，尤其是对于复杂的强化学习需要解决的几大困难，第二部分介绍Ray框架是如何解决这些问题的。<sup><a href="#fn_1" id="reffn_1">1</a></sup> <sup><a href="#fn_2" id="reffn_2">2</a></sup><br><a id="more"></a></p><h2 id="问题需要解决"><a href="#问题需要解决" class="headerlink" title="问题需要解决"></a>问题需要解决</h2><ol><li>低延迟</li><li>高吞吐量</li><li>动力学任务创建（Dynamics task creation）</li><li>任务的多样性（不同时间尺度，不同能量消耗尺度）</li><li>数据流之间的相关性</li><li>容错性</li><li>Debug和其他一些监视要求。</li></ol><h2 id="动力学计算图"><a href="#动力学计算图" class="headerlink" title="动力学计算图"></a>动力学计算图</h2><p>在Ray的框架里，采取了一种成为动力学计算图(Dynamics task graph computation)的计算模式。在Ray里面有三个主要节点：Task, Actor, Data Object。Task用来完成stateless的计算，Actor用来做stateful的计算，Data Object用来存储数据。那么很容易想象这个三者之间都存在某种序的关系，或者所谓的动力学关系。比如一个Task需要另一个Task的输出作为输入，同时随着计算的进行，数据也在不断的更新迭代，那么数据流的方向也具有动力学意义。对于Actor，Ray将其内部的状态的改变也转换成一个有方向的动力学过程。这有点类似把一个自循环的图展开计算。这样Ray的计算图就完全转化成一个无状态的动力学计算图。</p><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>这边我们介绍Ray是如何架构整个过程来完成上面的计算的并解决问题的。</p><p><img src="arche.png" alt=""></p><p>avatar: /figures/ray/arche.png<br>整个架构分为两层:App Layer + System Layer.</p><ul><li>App Layer <ul><li>Driver -&gt; 用来执行用户程序</li><li>Actor -&gt; 用来执行有状态的程序</li><li>Worker -&gt; 用来执行无状态的程序</li></ul></li></ul><ul><li>System Layer<ul><li>Global Control State(GCS) -&gt; Ray独特的设计<ul><li>解耦lineage和system components，这样在解决容错问题的时候又保证了性能。</li><li>把metadata存在GCS里而不是向以往的构架存在scheduler里，这样就解耦了task dispatch和task schedule，保证了性能.</li></ul></li><li>Bottom-up Distributed Scheduler -&gt; 为了满足低延迟调度任务，Ray将scheduler分成了两层。<ul><li>Local Scheduler</li><li>Global Scheduler<br>这样形成先global再local的自下而上的调度方式。</li></ul></li><li>In-Memory Distributed Object Store -&gt; 为了降低延迟保证吞吐量，采用了共享内存的存储方式，来降低overhead。</li></ul></li></ul><p><sup><a href="#fn_1" id="reffn_1">1</a></sup> Moritz, Philipp, et al. “Ray: A distributed framework for emerging {AI} applications.” 13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18). 2018.<br><sup><a href="#fn_2" id="reffn_2">2</a></sup> Nishihara, Robert, et al. “Real-time machine learning: The missing pieces.” Proceedings of the 16th Workshop on Hot Topics in Operating Systems. 2017.</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
